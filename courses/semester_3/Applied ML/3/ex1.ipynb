{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zoHDWoypC3PL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.1.3)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp39-cp39-win_amd64.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2023.3)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2023.3.23)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.16.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
            "Requirement already satisfied: wrapt in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ant\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ------------------------- -------------- 0.9/1.5 MB 30.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 23.7 MB/s eta 0:00:00\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "   ---------------------------------------- 0.0/472.7 kB ? eta -:--:--\n",
            "   --------------------------------------- 472.7/472.7 kB 28.9 MB/s eta 0:00:00\n",
            "Downloading gensim-4.3.3-cp39-cp39-win_amd64.whl (24.0 MB)\n",
            "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.5/24.0 MB 10.5 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 2.5/24.0 MB 26.0 MB/s eta 0:00:01\n",
            "   ------ --------------------------------- 4.0/24.0 MB 28.0 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 4.7/24.0 MB 27.3 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 5.1/24.0 MB 21.7 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 5.6/24.0 MB 19.8 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 6.3/24.0 MB 20.1 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 7.5/24.0 MB 19.9 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 8.9/24.0 MB 21.0 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 9.2/24.0 MB 19.5 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 10.5/24.0 MB 20.5 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 12.0/24.0 MB 20.5 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 13.4/24.0 MB 21.1 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 13.9/24.0 MB 20.5 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 15.1/24.0 MB 21.1 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 16.5/24.0 MB 23.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 17.8/24.0 MB 24.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 18.6/24.0 MB 23.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 20.2/24.0 MB 26.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 21.7/24.0 MB 26.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 23.1/24.0 MB 26.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.0/24.0 MB 26.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 24.0/24.0 MB 23.4 MB/s eta 0:00:00\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
            "   ---------------------------------------- 116.3/116.3 kB 7.1 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
            "   ---------------------------------------- 0.0/447.4 kB ? eta -:--:--\n",
            "   --------------------------------------- 447.4/447.4 kB 14.1 MB/s eta 0:00:00\n",
            "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
            "   ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 133.4/133.4 kB 8.2 MB/s eta 0:00:00\n",
            "Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl (25.1 MB)\n",
            "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 1.4/25.1 MB 30.7 MB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 3.0/25.1 MB 32.6 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 4.5/25.1 MB 35.8 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 5.9/25.1 MB 34.6 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 7.5/25.1 MB 34.4 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 9.1/25.1 MB 32.3 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 10.4/25.1 MB 31.2 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 11.9/25.1 MB 32.7 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 13.1/25.1 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 14.5/25.1 MB 31.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 15.7/25.1 MB 31.1 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 17.0/25.1 MB 29.8 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 18.5/25.1 MB 29.8 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 19.7/25.1 MB 28.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.0/25.1 MB 28.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 22.4/25.1 MB 28.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 23.7/25.1 MB 28.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.1/25.1 MB 29.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.1/25.1 MB 29.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 25.1/25.1 MB 26.2 MB/s eta 0:00:00\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 64.9/64.9 kB 3.4 MB/s eta 0:00:00\n",
            "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
            "   ---------------------------------------- 0.0/61.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 61.4/61.4 kB ? eta 0:00:00\n",
            "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 0.0/78.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
            "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
            "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
            "Installing collected packages: xxhash, tqdm, smart-open, requests, pyarrow, dill, click, nltk, multiprocess, huggingface-hub, gensim, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "Successfully installed click-8.1.7 datasets-3.0.2 dill-0.3.8 gensim-4.3.3 huggingface-hub-0.26.1 multiprocess-0.70.16 nltk-3.9.1 pyarrow-17.0.0 requests-2.32.3 smart-open-7.0.5 tqdm-4.66.5 xxhash-3.5.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn pandas nltk datasets gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV20ik8CL_iO"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_jIVgXAFAV2A"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "SUBSET_RATIO = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7D7AvR1OHGK"
      },
      "source": [
        "## Load IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "I-aMpHQqMI8w"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_dataset = (\n",
        "    dataset[\"train\"]\n",
        "    .shuffle(SEED)\n",
        "    .select(range(int(len(dataset[\"train\"]) * SUBSET_RATIO)))\n",
        ")\n",
        "test_dataset = (\n",
        "    dataset[\"test\"]\n",
        "    .shuffle(SEED)\n",
        "    .select(range(int(len(dataset[\"test\"]) * SUBSET_RATIO)))\n",
        ")\n",
        "\n",
        "df = train_dataset.to_pandas()\n",
        "df.head()\n",
        "\n",
        "df_test = test_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "0    129\n",
              "1    121\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsge6xhSOLVR"
      },
      "source": [
        "## Load English Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BYA0LKtfOP3T"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Ant\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ant\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTSyHjRKAq7u"
      },
      "source": [
        "## Task: Create a function that given embeddings and labels, trains a classifier and returns the predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "alfm0DnqAu7_"
      },
      "outputs": [],
      "source": [
        "import numpy.typing as npt\n",
        "import numpy as np\n",
        "from typing import Iterable\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def fit_predict(train_embeddings: npt.NDArray[float], train_labels: Iterable[int], test_embeddings: npt.NDArray[float]) -> npt.NDArray[float]:\n",
        "  clf = RandomForestClassifier(random_state=0)\n",
        "  clf.fit(train_embeddings, train_labels)\n",
        "  predictions = clf.predict(test_embeddings)\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWqIOM4qNDYf"
      },
      "source": [
        "# Part 1: Historical Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMFf63W6PjSH"
      },
      "source": [
        "### Bag-of-Words (BoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sRzVQO-Qper"
      },
      "source": [
        "#### Task: Compute the corpus embeddings using a Bag-of-Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TM1yCA8tNHnx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt4spFaAQoAn"
      },
      "source": [
        "#### Task: Use `fit_predict` to train a classifier using the BoW embeddings. Display the accuracy and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cJpfGS-jB773"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=stopwords)\n",
        "train_embeddings = vectorizer.fit_transform(df[\"text\"])\n",
        "test_embeddings = vectorizer.transform(df_test[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = fit_predict(train_embeddings, df[\"label\"], test_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.72\n",
            "f1: 0.7083333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print(f\"accuracy: {accuracy_score(df_test['label'], predictions)}\")\n",
        "print(f\"f1: {f1_score(df_test['label'], predictions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvoqoeTfTpcV"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa2Uda1TT6VX"
      },
      "source": [
        "#### Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmk39avn8bjp"
      },
      "source": [
        "Now, let's implement the TF-IDF computation ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvwa4oHq8gaO"
      },
      "source": [
        "##### Step 1: Tokenization and Count Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4KSuzJ98kHM"
      },
      "source": [
        "First, we need to tokenize the documents and create a term-frequency (TF) matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTR7kFBU8USd"
      },
      "outputs": [],
      "source": [
        "def tokenize_documents(documents: list[str]) -> list[list[str]]:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSLkKsY-CCuv"
      },
      "source": [
        "Check that your function is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgMTejteSZXL"
      },
      "outputs": [],
      "source": [
        "tokenized_documents = tokenize_documents(df['text'])\n",
        "\n",
        "assert type(tokenized_documents) == list\n",
        "assert type(tokenized_documents[0]) == list\n",
        "assert type(tokenized_documents[0][0]) == str\n",
        "assert len(tokenized_documents) == len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnvhOjBk8qm_"
      },
      "source": [
        "###### Step 2: Build Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRMGORZe8vBf"
      },
      "source": [
        "Create a vocabulary of all unique words in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgrAv0_W8npP"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(tokenized_documents) -> Iterable[str]:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsnoy8e6CLxb"
      },
      "source": [
        "Check that your function is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc-Jw7_3VBFO"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "\n",
        "vocabulary = build_vocabulary(tokenized_documents)\n",
        "\n",
        "assert isinstance(vocabulary, Iterable)\n",
        "assert len(vocabulary) == len(set(vocabulary))\n",
        "assert len(vocabulary) == 33924 # Can be different depending on the tokenization method used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzjtb0ho87RP"
      },
      "source": [
        "###### Step 3: Compute Term Frequencies (TF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-lNgbF69BsA"
      },
      "source": [
        "Compute the term frequency matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhfjIVMx9Dea"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "\n",
        "def compute_tf(tokenized_documents: list[list[str]], vocabulary: Iterable[str]) -> npt.NDArray[np.float64]:\n",
        "    # Get vocabulary size\n",
        "    vocab_size = len(vocabulary)\n",
        "\n",
        "    # Create a mapping from word to index\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Initialize term frequency matrix\n",
        "    tf_matrix = np.zeros((len(tokenized_documents), vocab_size))\n",
        "\n",
        "    # Compute term frequencies\n",
        "    # Fill me\n",
        "\n",
        "    return tf_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_HSVo-oCbC5"
      },
      "source": [
        "Check that your function is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaLoBh9WWlBp"
      },
      "outputs": [],
      "source": [
        "tf = compute_tf(tokenized_documents, vocabulary)\n",
        "\n",
        "assert tf.shape == (len(df), len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeFbEc2Y9Ge3"
      },
      "source": [
        "##### Step 4: Compute Inverse Document Frequencies (IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp2YAGwc9KPn"
      },
      "source": [
        "Compute the document frequency for each term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmm6tJrR9L1B"
      },
      "outputs": [],
      "source": [
        "def compute_df(tf_matrix: npt.NDArray[np.float32]) -> npt.NDArray[np.float32]:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uqWx61eCft-"
      },
      "source": [
        "Check that your function is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiCQW6pbccIX"
      },
      "outputs": [],
      "source": [
        "df_counts = compute_df(tf)\n",
        "\n",
        "assert df_counts.shape == (len(vocabulary),)\n",
        "assert np.all(df_counts > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UTDBKZcO3sU"
      },
      "source": [
        "Compute the inverse document frequency for each term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV0ol2vaO7h-"
      },
      "outputs": [],
      "source": [
        "def compute_idf(tf_matrix: npt.NDArray[np.float32]) -> npt.NDArray[np.float32]:\n",
        "    number_of_documents = tf_matrix.shape[0]\n",
        "    df_counts = compute_df(tf_matrix)\n",
        "\n",
        "    # Fill me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaskjs_dCoIH"
      },
      "source": [
        "Check that your function is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OnhkH5mdzxg"
      },
      "outputs": [],
      "source": [
        "idf = compute_idf(tf)\n",
        "\n",
        "assert idf.shape == (len(vocabulary), )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQv5MvN_PSsi"
      },
      "source": [
        "##### Step 5: Compute TF-IDF Matrix and normalize to unit length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynjwoRJFPnnL"
      },
      "source": [
        "Put everything together and compute the TF-IDF of a corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ujyuqqwQbA4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def tf_idf(documents: list[str]):\n",
        "    # Tokenize documents\n",
        "    tokenized_documents = tokenize_documents(documents)\n",
        "\n",
        "    # Build vocabulary\n",
        "    vocabulary = build_vocabulary(tokenized_documents)\n",
        "\n",
        "    # Compute TF\n",
        "    tf = compute_tf(tokenized_documents, vocabulary)\n",
        "\n",
        "    # Compute IDF\n",
        "    idf = compute_idf(tf)\n",
        "\n",
        "    # Compute TF-IDF\n",
        "    # Fill me\n",
        "\n",
        "    # Normalize TF-IDF\n",
        "    # Fill me (why do we need to normalize?)\n",
        "\n",
        "    return tf_idf_matrix_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g089vr_DCzen"
      },
      "source": [
        "Check that your function is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBn_SAxQe-4u"
      },
      "outputs": [],
      "source": [
        "tf_idf_matrix = tf_idf(df['text'])\n",
        "\n",
        "assert tf_idf_matrix.shape == (len(df), len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HvpSRVcfIie"
      },
      "source": [
        "Task: Use fit_predict to train a classifier using TF-IDF embeddings. Display the accuracy and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5SAv7VefJ8b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHltqhYtik1Y"
      },
      "source": [
        "# Part 2: Word Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIDrtYqliyLC"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "\n",
        "glove = gensim.downloader.load(\"glove-wiki-gigaword-100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ASj_8Qj9OU"
      },
      "source": [
        "### Task: Create a function that computes a document embedding using GloVe  embeddings and a pooling function of your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FVRLxFKknYu"
      },
      "outputs": [],
      "source": [
        "def document_embedding(document: str) -> npt.NDArray[np.float32]:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZed3154DLnr"
      },
      "source": [
        "Check that your function is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qreiq9Emk9Ro"
      },
      "outputs": [],
      "source": [
        "first_document = df['text'].iloc[0]\n",
        "first_document_embedding =  document_embedding(first_document)\n",
        "\n",
        "assert first_document_embedding.shape == (glove.vector_size,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjcCNvxPoWUH"
      },
      "source": [
        "## Task: Train a classifier using GloVe document embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaNwCsV7okBG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
