{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 3 - Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions To Run This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On your local machine (Assuming MacOS or Linux based distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a folder: `mkdir sheet3`\n",
    "2. Go to the folder `cd sheet3`\n",
    "3. Install all the necessary dependencies (we assume you already have python and pip) by running the following command:\n",
    "    `pip install pyspark pandas matplotlib numpy scikit-learn`\n",
    "4. Download the files **After_Pre_Processing.xlsx** and **Before_Pre_Processing.xlsx** from moodle if you don't have them\n",
    "5. Put the **After_Pre_Processing.xlsx** and **Before_Pre_Processing.xlsx** in the folder **sheet3** you have just created\n",
    "6. Put this notebook **sheet3_exercise4.ipynb** in the same folder **sheet3**\n",
    "7. Make sure the image **metrics.png** is also in the same folder\n",
    "8. Now you can run this notebook on *Jupyter* or *VSCode* by clicking \"Run All\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On HPC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook on HPC you will first need to setup Jupyter on HPC by following these steps:\n",
    "\n",
    "1. Log into your HPC instance by using the command: `ssh -p 8022 <your-login>@access-iris.uni.lu` (replace your-login with your HPC login)\n",
    "2. Once logged in, initialize a session using `si`\n",
    "3. Create a directory inside HPC: `mkdir sheet3`\n",
    "4. Go to the directory: `cd sheet3`\n",
    "5. Load python: `module load lang/Python`\n",
    "6. Create a Python environment for this activity: `python -m venv jupyter_env`\n",
    "7. Activate the environment: `source jupyter_env/bin/activate`\n",
    "8. Install/Upgrade pip: `python -m pip install --upgrade pip`\n",
    "9. Install Jupyter: `python -m pip install jupyter ipykernel`\n",
    "10. Install all the required dependencies: `pip install pyspark pandas matplotlib numpy scikit-learn`\n",
    "11. Register our environment to the Jupyter kernel: `python -m ipykernel install --sys-prefix --name jupyter_env`\n",
    "12. Run the following command to launch a jupyter server on HPC: `jupyter notebook --no-browser --port=8888 --NotebookApp.token='' --NotebookApp.password='' -NotebookApp.disable_check_xsrf=True --ip=0.0.0.0`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go on your local machine, open a new terminal tab and do the following:\n",
    "1. Copy files from local to HPC, on your local machine within the directory where you have the .ipynb and .xlsx files stored run the following commands:\n",
    "`scp -P 8022 sheet3_exercise4.ipynb <your-login>@access-iris.uni.lu:~/sheet3`\n",
    "`scp -P 8022 After_Pre_Processing.xlsx <your-login>@access-iris.uni.lu:~/sheet3`\n",
    "`scp -P 8022 Before_Pre_Processing.xlsx <your-login>@access-iris.uni.lu:~/sheet3`\n",
    "2. Launch the jupyter notebook on localhost by running: `ssh -p 8022 -NL 8888:iris-XXX:8888 <your-login>@access-iris.uni.lu` and replace XXX by the number of the iris cluster provided to you on HPC for example I have on HPC *(jupyter_env) 0 [omahfoud@iris-055 sheet3](3487277 1N/T/1CN)$* so I would replace XXX by *055*\n",
    "3. On your machine open your favorite browser and go to `localhost:8888` or `http://127.0.0.1:8888/` this should open your Jupyter interface\n",
    "4. Click on **sheet3_exercise4.ipynb** then click on **Run All*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures for Multiclass Classification based on Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After carefuly reading the paper Sokolova, M., Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. Information Processing and Management, 45, p. 427-437. I have seen that in page 430 the author summarizes in Table 3 Measures for multi-class classification based on a generalization of the measures of Table 1 (Confusion matrix for binary classification and the corresponding array representation used in this paper.) for many classes <br>\n",
    "Ci: tpi are true positive for Ci, and fpi – false positive, fni – false negative, and tni – true negative counts respectively. <br>\n",
    "l and M indices represent micro- and macro-averaging. <br>\n",
    "The table in metrics.png (shown below) summarize the metrics aforementionned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefine all the previously implemented metrics in Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will review the Classification section of exercise 2 while defining new metrics based on the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Imports and Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: pandas in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (3.8.4)\n",
      "Requirement already satisfied: numpy in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (1.22.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/othmane123/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark pandas matplotlib numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initializing a spark session\n",
    "spark = SparkSession.builder.appName(\"sheet3_exercise2\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+---------+-------------------+---------+---------+---------+---------+--------------------+--------------------+------+------+-------------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+--------------------+------+------+------+-------------------+------+------+------+------+------+------------------+------------------+---------+-----+-----+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|   USER|S1Q01nor|S1Q05|S1Q061nor|        S1Q06_P1nor|S1Q062nor|S1Q063NOR|S1Q065NOR|S1Q067NOR|           S1Q071nor|           S1Q072nor|S1Q081|S1Q082|            S1Q09_b|S2Q01_a_201501|S2Q01_a_201801|S2Q01_a_201502|S2Q01_a_201802|S2Q01_a_201503 |S2Q01_a_201803|S2Q01_a_201504|S2Q01_a_201804|S2Q01_a_201505|S2Q01_a_201805|S2Q01_a_201506|S2Q01_a_201806|S2Q01_a_201507|S2Q01_a_201807|S2Q01_a_201508|S2Q01_a_201808|S2Q01_a_201509|S2Q01_a_201809|S2Q01_a_201510|S2Q01_a_201810|S2Q01_b_201511|S2Q01_b_201811|S2Q01_b_201512|S2Q01_b_201812|S2Q01_b_201513|S2Q01_b_201813|S2Q01_b_201514|S2Q01_b_201814|S2Q01_b_201515|S2Q01_b_201815|S2Q01_b_201516|S2Q01_b_201816|S2Q01_b_201517|S2Q01_b_201817|S2Q01_b_201518|S2Q01_b_201818|S2Q01_b_201519|S2Q01_b_201819|S2Q02_01|S2Q02_02|S2Q02_03|S2Q02_04|S2Q02_05|S2Q02_06|S2Q02_07|S2Q02_08|S2Q02_09|S2Q02_10|S2Q02_11|S3Q01_20151|S3Q01_20181|S3Q01_20152|S3Q01_20182|S3Q02_01|S3Q02_02|S3Q02_03|S3Q02_04|S3Q02_05|S3Q02_06|S3Q02_07|S3Q02_08|S3Q02_09|S3Q02_10|S3Q03-AL|S4Q01|           S4Q011_AL|S4Q021|S4Q022|S4Q023|             S4Q024|S4Q025|S4Q026|S4Q031|S4Q041|S4Q051|            S4Q061|            S4Q071|S5Q010-AL|S5Q02|S5Q03|             S5Q04|S5Q05_a01|S5Q05_a02|S5Q05_a03|S5Q05_a04|S5Q05_a05|S5Q05_a06|S5Q05_a24|S5Q05_a07|S5Q05_a08|S5Q05_a09|S5Q05_a10|S5Q05_a11|S5Q05_a12|S5Q05_b13|S5Q05_b14|S5Q05_b15|S5Q05_b16|S5Q05_b17|S5Q05_b18|S5Q05_b19|S5Q05_b20|S5Q05_b21|S5Q05_b22|\n",
      "+-------+--------+-----+---------+-------------------+---------+---------+---------+---------+--------------------+--------------------+------+------+-------------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+--------------------+------+------+------+-------------------+------+------+------+------+------+------------------+------------------+---------+-----+-----+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|P100017|     0.0|    1|      1.0|           0.453125|      1.0|      1.0|      1.0|      1.0|0.005732484076433121| 0.00784313725490196|   0.9|   0.1| 0.7142857142857143|          0.25|          0.25|          0.25|          0.25|           0.25|          0.25|          0.25|          0.25|          0.25|           0.5|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|           0.5|           0.5|          0.25|          0.25|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|     0.0|     0.0|     0.0|    0.25|     0.0|     0.0|    0.25|     0.5|     0.0|     0.0|     0.0|       0.25|       0.25|       0.25|       0.25|     0.5|     0.5|    0.25|    0.75|    0.25|    0.25|    0.25|    0.25|    0.25|    0.25|     0.7|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0| 0.25|0.6666666666666666|      1.0|      1.0|      1.0|      1.0|      1.0|      0.5|      1.0|      1.0|      1.0|      0.5|      0.5|      0.5|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|\n",
      "|P100131|     0.0|    0|      0.0|0.48333333333333334|      0.5|      0.5|      0.5|      0.0|0.006369426751592357|  0.0196078431372549|  0.87|  0.13| 0.5714285714285714|          0.75|          0.75|          0.75|          0.75|           0.75|          0.75|          0.25|          0.75|           0.0|           1.0|           0.5|           1.0|          0.75|          0.75|           0.0|          0.75|           0.0|          0.75|           0.0|           1.0|           0.0|          0.75|           0.0|           0.5|           0.0|          0.75|           0.0|          0.75|           0.0|          0.25|           0.0|          0.75|           0.0|          0.25|           0.0|           1.0|           0.0|          0.25|     1.0|     1.0|    0.75|     0.5|    0.75|     0.5|     1.0|     1.0|     1.0|     1.0|    0.75|       0.25|       0.75|       0.25|       0.75|     1.0|     1.0|    0.75|     1.0|     0.5|     1.0|     1.0|     0.5|    0.75|     1.0|     0.0|    0|              9.8E-5|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0| 0.75|0.3333333333333333|     0.75|      1.0|     0.75|      1.0|      1.0|     0.25|     0.75|      1.0|      1.0|      0.5|      0.5|      0.0|      0.5|      0.0|      0.0|      0.0|     0.75|      1.0|      0.5|     0.75|      1.0|     0.25|      0.5|\n",
      "|P100137|     0.0|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001273885350318...| 0.00392156862745098|   0.7|   0.3| 0.2857142857142857|           0.5|           0.5|          0.25|          0.25|           0.25|          0.25|           0.5|           0.5|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|          0.75|          0.75|          0.25|          0.25|           0.0|           0.0|          0.25|          0.25|           0.0|           0.0|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|          0.25|          0.25|           0.0|           0.0|           0.5|           0.5|           0.0|           0.0|     0.0|     0.0|    0.25|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|        0.5|        0.5|        0.5|        0.5|    0.75|    0.75|     0.5|    0.75|    0.75|     0.5|     0.5|     0.5|     0.5|    0.75|     0.3|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|      0.0|      0.0|      0.0|      1.0|      1.0|      0.0|     0.25|      0.5|     0.25|     0.25|     0.25|      0.0|      1.0|     0.25|      0.5|     0.25|      0.5|     0.75|     0.25|     0.25|     0.75|      0.0|     0.25|\n",
      "|P100169|     0.1|    1|      1.0| 0.4270833333333333|      1.0|      1.0|      1.0|      1.0|6.369426751592356E-4| 0.00196078431372549|   0.9|   0.1|                0.0|           0.0|          0.25|           0.0|          0.25|            0.0|          0.25|           0.0|          0.25|           0.0|          0.25|           0.0|          0.25|           0.5|          0.25|          0.25|          0.25|           0.0|          0.25|          0.25|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|    0.25|    0.25|    0.25|     0.5|    0.25|     0.5|    0.25|     0.5|     0.5|     0.5|    0.25|       0.25|        0.5|       0.25|        0.5|    0.25|    0.25|     0.5|     0.5|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0|  0.5|0.6666666666666666|     0.25|      0.0|      0.0|      0.0|      0.0|      0.0|      0.5|      0.5|      0.5|      0.5|     0.25|      0.0|      0.5|      0.5|      0.5|      0.5|     0.25|      0.5|      0.5|     0.25|     0.25|     0.25|      0.0|\n",
      "|P100309|     0.0|    0|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001273885350318...| 0.00392156862745098|   0.7|   0.3| 0.2857142857142857|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.0|           0.0|     0.5|     0.5|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     0.0|        0.5|        0.5|        0.5|        0.5|     1.0|     1.0|     1.0|     1.0|     1.0|    0.75|    0.75|    0.75|    0.75|     1.0|     0.0|    1|            2.667E-5|   0.7|   0.3|   0.0|                0.0|   0.0|   0.0|   0.2| 0.125|   0.4|0.6666666666666666|0.3333333333333333|      0.0|    0| 0.25|0.6666666666666666|      0.0|      0.0|     0.25|      1.0|      1.0|      1.0|      1.0|      0.5|      0.5|     0.75|     0.25|      0.5|      1.0|      0.5|      1.0|      1.0|      0.5|      1.0|      1.0|      0.0|      1.0|      1.0|      0.0|\n",
      "|P100331|     0.1|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|3.184713375796178E-4| 0.01019607843137255|  0.98|  0.02|                0.0|           0.5|           0.5|           0.5|           0.5|            0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|    0.25|    0.25|     0.5|    0.25|    0.25|    0.25|     0.5|     0.5|     0.5|    0.75|     1.0|        0.5|        0.5|        0.5|        0.5|     0.5|     1.0|     0.5|     1.0|     0.5|    0.75|     0.5|     0.5|    0.75|    0.75|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|     0.25|     0.25|      0.5|     0.75|      1.0|     0.75|      0.5|     0.75|      0.5|     0.25|      0.0|      0.0|      1.0|     0.25|      1.0|     0.75|      0.5|      0.5|     0.25|      0.5|      1.0|      1.0|      0.5|\n",
      "|P100333|     0.0|    0|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.003821656050955414|                 0.0|  0.75|  0.25| 0.5714285714285714|          0.75|          0.75|          0.75|          0.75|           0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|           1.0|           1.0|          0.75|          0.75|           1.0|           1.0|           1.0|           1.0|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|    0.75|    0.75|    0.75|     1.0|     1.0|    0.75|     1.0|     1.0|    0.75|    0.75|     0.5|        0.5|        0.5|        0.5|        0.5|     1.0|    0.75|     1.0|     1.0|    0.75|    0.75|     1.0|    0.75|     0.5|     1.0|     0.2|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0|  0.0|0.6666666666666666|     0.75|     0.75|      1.0|      0.5|      0.5|      0.5|      1.0|      1.0|      1.0|      1.0|     0.75|     0.75|     0.75|     0.75|     0.75|     0.75|      0.5|     0.75|     0.75|      0.5|      0.5|      0.5|      0.5|\n",
      "|P100491|     0.0|    1|      1.0| 0.4270833333333333|      0.5|      0.5|      0.5|      0.5|0.041401273885350316| 0.12254901960784313|   0.9|   0.1|                1.0|           0.5|           1.0|           0.5|          0.75|            0.5|           1.0|          0.25|           1.0|          0.25|          0.75|           1.0|           0.5|           0.0|           1.0|           0.0|           1.0|           0.0|          0.75|           0.0|           1.0|           0.0|           1.0|          0.25|          0.75|           1.0|           1.0|           0.5|          0.75|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|    0.75|    0.75|     0.5|    0.75|    0.75|     1.0|    0.75|     1.0|     0.5|     1.0|     1.0|        0.5|        0.5|        0.5|        0.5|    0.75|     1.0|     0.5|     0.5|     1.0|    0.75|    0.75|    0.75|    0.25|     0.5|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0|  0.5|0.6666666666666666|      0.0|     0.25|      0.5|      1.0|      1.0|      0.5|      0.5|      0.5|      0.5|     0.25|     0.25|      0.0|     0.75|      0.0|      0.5|      0.5|     0.75|      0.5|      0.0|      0.5|     0.25|      0.0|      0.0|\n",
      "|P100513|     0.0|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|                 1.0|                 1.0|   0.7|   0.3| 0.5714285714285714|           1.0|           1.0|          0.75|           1.0|            1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           0.5|           1.0|          0.75|           1.0|           0.5|           1.0|           0.5|           1.0|           0.5|          0.75|           0.5|           1.0|           0.5|          0.75|           0.5|          0.75|          0.75|          0.75|           0.5|           0.5|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|          0.75|          0.75|    0.75|     0.5|     1.0|     1.0|     1.0|    0.75|     1.0|     1.0|     1.0|     1.0|     1.0|        0.5|       0.75|        0.5|       0.75|    0.75|    0.75|    0.75|     1.0|     1.0|     1.0|     1.0|    0.75|    0.75|    0.75|     0.2|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.2|    0|  0.0|0.6666666666666666|      0.5|      0.5|      0.5|     0.25|     0.75|      0.5|     0.75|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|     0.75|     0.75|     0.75|     0.75|     0.75|      0.5|     0.75|      0.5|\n",
      "|P100579|     0.0|    1|      1.0| 0.2708333333333333|      1.0|      1.0|      1.0|      1.0|0.001910828025477707|0.005882352941176...|   0.4|   0.6|                0.0|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           0.0|           0.0|           1.0|           1.0|           1.0|           1.0|           0.0|           0.0|           1.0|           1.0|           0.0|           0.0|           1.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|     1.0|     0.0|     0.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|        0.5|        0.5|        0.5|        0.5|     1.0|     0.5|     1.0|     1.0|     0.5|     1.0|     1.0|     1.0|     1.0|     1.0|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    1| 0.25|0.6666666666666666|      0.5|      0.5|      0.5|      1.0|      1.0|      0.5|      0.5|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      0.5|      1.0|      1.0|      0.5|      1.0|      1.0|      0.5|\n",
      "|P100767|     0.0|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001910828025477707|0.004901960784313725|   0.9|   0.1|0.14285714285714285|          0.25|          0.25|          0.25|          0.25|           0.25|          0.25|          0.25|          0.25|           0.5|           0.5|          0.25|          0.25|          0.25|          0.75|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|     0.5|     0.5|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|        0.5|        0.5|        0.5|        0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0| 0.25|0.6666666666666666|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|\n",
      "|P100849|     0.0|    0|      0.0|            0.53125|      0.5|      0.5|      0.0|      0.0|0.001592356687898...|0.004901960784313725|   0.8|   0.2|0.42857142857142855|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           1.0|           1.0|          0.25|          0.25|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           0.0|           0.0|          0.75|          0.75|           1.0|           1.0|          0.75|          0.75|           0.5|           0.5|          0.75|          0.75|          0.75|          0.75|           0.0|          0.25|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|    0.25|    0.25|     0.5|     0.5|    0.75|    0.75|     1.0|     1.0|    0.75|    0.75|     0.5|        0.5|        0.5|        0.5|        0.5|     1.0|     1.0|     0.5|    0.75|    0.75|    0.75|     1.0|     0.5|     0.5|     0.5|     0.0|    1|                 0.0|   0.3|   0.0|   0.2| 0.7142857142857143|   0.0|   0.0|   0.2|  0.25|   1.0|0.6666666666666666|0.3333333333333333|      0.0|    0| 0.25|0.6666666666666666|      0.0|      0.0|      0.0|     0.75|      1.0|      1.0|      1.0|      1.0|     0.75|     0.75|     0.75|      0.5|      1.0|      0.5|      0.5|      1.0|      1.0|      0.5|      0.5|     0.75|     0.75|     0.75|      0.5|\n",
      "|p100997|     1.0|    0|      0.5| 0.4791666666666667|      0.5|      0.0|      0.5|      1.0|0.007961783439490446|0.022549019607843137|  0.82|  0.18|                1.0|           0.5|           1.0|          0.25|          0.75|           0.25|          0.25|          0.25|           0.5|           0.0|           0.0|           0.5|           0.5|           0.0|           0.0|           0.5|           1.0|           0.0|           0.0|           0.0|           0.0|           0.5|           1.0|           0.0|           0.0|           0.0|           0.0|          0.25|           0.5|           0.5|           0.5|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|     0.5|     0.0|     0.5|     0.5|    0.75|    0.25|     0.5|     1.0|     0.5|     0.5|     0.5|        0.5|        0.5|        0.5|        0.5|     0.5|    0.75|     0.0|    0.75|    0.75|    0.75|     0.5|     0.5|     0.5|    0.25|     0.3|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|      0.0|      0.0|     0.75|     0.75|     0.75|      0.0|      0.5|     0.75|     0.75|     0.75|      0.5|      0.0|      0.5|      0.0|     0.75|      0.5|      0.5|      0.0|      0.0|     0.25|      0.5|      0.0|      0.0|\n",
      "|P101285|     0.0|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001910828025477707|0.004901960784313725|   0.8|   0.2|0.14285714285714285|           0.5|           0.5|           0.5|           0.5|           0.25|           0.5|          0.25|          0.25|           0.5|          0.25|          0.25|           0.5|          0.75|           0.5|           0.5|          0.75|           0.5|           0.5|          0.25|           0.5|          0.25|           0.5|           0.5|          0.75|          0.25|           0.5|           0.5|          0.25|          0.25|           0.5|           0.5|          0.75|          0.25|           0.5|           0.5|          0.25|           0.5|          0.25|     0.5|    0.75|    0.25|     0.5|    0.25|     0.5|    0.25|    0.25|     0.5|    0.25|     0.5|        0.5|        0.5|        0.5|        0.5|     0.5|    0.75|     0.5|    0.75|    0.75|     0.5|     0.5|     0.5|    0.75|    0.75|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|     0.25|     0.25|     0.25|      0.5|     0.75|     0.25|     0.75|      1.0|     0.75|      1.0|      0.5|      0.5|     0.75|      0.5|     0.75|     0.25|     0.25|     0.75|      0.5|     0.25|      1.0|      1.0|     0.75|\n",
      "|P101363|     0.0|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      1.0|      0.5|0.001592356687898...| 0.00392156862745098|   0.4|   0.6|0.14285714285714285|           0.5|           0.5|           0.5|           0.5|            0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|           0.5|          0.75|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|          0.25|          0.25|          0.25|    0.25|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|    0.75|    0.75|     1.0|     1.0|       0.25|        0.5|       0.25|        0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.0|    1|              4.0E-4|   0.8|  0.07|  0.03|0.07142857142857142|  0.05|   0.0|   0.2| 0.125|   0.2|0.1111111111111111|0.3333333333333333|      0.0|    0|  0.5|0.6666666666666666|      0.5|      0.5|      0.5|      0.5|      0.5|      0.0|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|     0.75|     0.75|     0.75|      0.5|      0.5|      0.5|      0.5|      0.5|     0.25|     0.25|\n",
      "|P101733|     0.0|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001273885350318...| 9.80392156862745E-4|   0.7|   0.3|0.14285714285714285|           0.5|           0.5|           0.5|           0.5|           0.75|          0.75|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|          0.75|          0.75|          0.25|          0.25|     1.0|    0.75|    0.75|    0.75|    0.75|    0.25|     0.5|    0.75|    0.75|    0.75|    0.75|        0.5|        0.5|       0.25|       0.25|     0.5|     0.0|     0.5|    0.75|    0.75|    0.75|    0.75|    0.75|    0.75|    0.75|     0.1|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0| 0.75|0.6666666666666666|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|     0.75|     0.75|     0.75|     0.75|      0.5|     0.75|     0.75|      1.0|      1.0|      1.0|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|\n",
      "|P101915|     0.0|    1|      1.0| 0.4270833333333333|      1.0|      0.5|      0.5|      1.0|9.554140127388535E-4| 9.80392156862745E-4|   0.8|   0.2|0.14285714285714285|          0.25|          0.25|          0.25|          0.25|            0.5|           0.5|          0.25|          0.25|          0.25|           0.5|          0.25|           0.5|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|           0.0|          0.25|           0.0|          0.25|           0.0|          0.25|          0.25|           0.5|          0.25|           0.5|          0.25|          0.25|           0.0|           0.0|           0.0|           0.0|          0.25|          0.25|          0.25|          0.25|     0.0|     0.0|     0.5|     0.5|     0.5|     0.5|    0.75|    0.75|    0.25|    0.75|    0.75|        0.5|       0.25|        0.5|       0.25|     0.5|    0.75|    0.25|    0.75|    0.25|     0.5|     0.5|    0.25|    0.25|    0.75|     0.1|    1|0.006666666666666667|   0.0|   0.5|   0.5|                0.0|   0.0|   0.0|   0.8| 0.125|   0.4|0.5555555555555556|0.3333333333333333|      0.0|    0|  0.0|0.6666666666666666|     0.25|     0.25|      0.5|      0.5|     0.75|      0.5|     0.75|     0.75|     0.75|     0.75|      0.5|      0.5|     0.75|     0.25|     0.75|      0.5|     0.25|     0.25|     0.25|     0.25|     0.25|     0.25|     0.25|\n",
      "|P102453|     0.0|    0|      1.0| 0.4270833333333333|      1.0|      0.5|      0.5|      1.0|0.005414012738853503|0.014705882352941176|   0.5|   0.5| 0.7142857142857143|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|          0.75|           1.0|           1.0|           1.0|          0.75|           1.0|          0.75|           1.0|          0.75|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|     1.0|    0.75|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|    0.75|     1.0|    0.75|        0.5|       0.75|        0.5|       0.75|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     0.1|    1|                0.02|   0.7|   0.0|   0.1|0.14285714285714285|   0.1|   0.0|   0.2|  0.25|   0.2|0.6666666666666666|0.3333333333333333|      0.3|    1| 0.25|0.6666666666666666|      0.5|      0.5|     0.75|     0.75|      1.0|      1.0|      1.0|      1.0|     0.75|     0.75|     0.75|      0.5|     0.75|      1.0|      1.0|      1.0|      1.0|     0.75|      0.5|      0.5|     0.75|      0.5|      0.5|\n",
      "|P102473|     0.1|    1|      0.5| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5| 0.00678343949044586| 0.01019607843137255|   0.6|   0.4|                0.0|           0.5|           0.5|           0.5|          0.25|            0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|          0.75|          0.75|          0.75|          0.75|           0.5|           0.5|          0.75|           0.5|           0.5|           0.5|          0.25|          0.25|           0.0|           0.0|          0.25|          0.25|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|    0.75|    0.25|    0.25|     0.5|     0.5|     0.5|     0.5|     1.0|    0.25|     1.0|     1.0|        0.5|        0.0|        0.0|        0.0|     0.5|    0.75|     0.5|    0.75|    0.75|    0.75|     0.5|    0.25|    0.25|    0.25|     0.3|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.2|    0| 0.25|0.6666666666666666|     0.25|      0.5|      0.0|      1.0|      1.0|     0.25|     0.75|     0.75|      0.0|      0.0|     0.75|     0.75|     0.75|      1.0|      1.0|      1.0|     0.75|      0.5|      0.5|      0.5|      0.5|      0.5|      0.0|\n",
      "|P102621|     0.0|    0|      0.0|            0.53125|      0.5|      0.5|      0.5|      0.0|0.007961783439490446| 0.00980392156862745|  0.72|  0.28|                1.0|          0.75|           1.0|          0.75|           1.0|            1.0|           1.0|           1.0|           1.0|          0.75|           1.0|          0.75|           1.0|           0.5|           1.0|          0.75|           1.0|          0.75|           1.0|          0.75|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|          0.75|           0.5|          0.75|           1.0|          0.75|           1.0|          0.75|           1.0|          0.75|          0.75|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|        0.5|        1.0|        0.5|        1.0|    0.75|    0.75|    0.75|     1.0|     1.0|     1.0|     1.0|     1.0|    0.75|     1.0|     0.0|    1|                 0.1|   0.5|   0.2|   0.3|                0.0|   0.0|   0.0|   0.2| 0.125|   0.2|0.5555555555555556|0.3333333333333333|      0.0|    0| 0.75|               1.0|     0.25|      0.5|      0.5|      1.0|      1.0|     0.75|     0.75|     0.75|     0.75|     0.75|      0.5|      1.0|      1.0|      1.0|      0.5|     0.75|      0.5|      0.5|      0.5|     0.75|      0.5|      0.5|      0.0|\n",
      "+-------+--------+-----+---------+-------------------+---------+---------+---------+---------+--------------------+--------------------+------+------+-------------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+--------------------+------+------+------+-------------------+------+------+------+------+------+------------------+------------------+---------+-----+-----+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading excel data to a spark RDD\n",
    "raw_data = pd.read_excel(\"After_Pre_Processing.xlsx\")\n",
    "spark_df = spark.createDataFrame(raw_data)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \"USER\" column\n",
    "spark_df = spark_df.drop('USER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that reducing the number of features and selecting only the most important ones has increased our error, therefore in our case fr this regression it was not useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier for Categorical Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataframe from our data and follow the same steps as above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.drop('USER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'S1Q061nor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(target, col(target) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        if df.schema[column].dataType == \"DoubleType\":\n",
    "            mean_value = df.select(col(column)).na.drop().groupBy().avg(column).first()[0]\n",
    "            df = df.fillna(mean_value, subset=[column])\n",
    "        else:\n",
    "            mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
    "            df = df.fillna(mode_value, subset=[column])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = fill_missing_values(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in spark_df.columns:\n",
    "    if spark_df.schema[column].dataType == \"StringType\":\n",
    "        spark_df = spark_df.withColumn(column, when(col(column).isNull(), \"\").otherwise(col(column)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define indexers and assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_indexed\").fit(spark_df)\n",
    "            for column in spark_df.columns if spark_df.schema[column].dataType == \"StringType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[col + \"_indexed\" if col + \"_indexed\" in spark_df.columns else col for col in spark_df.columns if col != target],\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "pipeline_model = pipeline.fit(spark_df)\n",
    "preprocessed_df = pipeline_model.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+-------------------+---------+---------+---------+---------+--------------------+--------------------+------+------+-------------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+--------------------+------+------+------+-------------------+------+------+------+------+------+------------------+------------------+---------+-----+-----+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------------+\n",
      "|S1Q01nor|S1Q05|S1Q061nor|        S1Q06_P1nor|S1Q062nor|S1Q063NOR|S1Q065NOR|S1Q067NOR|           S1Q071nor|           S1Q072nor|S1Q081|S1Q082|            S1Q09_b|S2Q01_a_201501|S2Q01_a_201801|S2Q01_a_201502|S2Q01_a_201802|S2Q01_a_201503 |S2Q01_a_201803|S2Q01_a_201504|S2Q01_a_201804|S2Q01_a_201505|S2Q01_a_201805|S2Q01_a_201506|S2Q01_a_201806|S2Q01_a_201507|S2Q01_a_201807|S2Q01_a_201508|S2Q01_a_201808|S2Q01_a_201509|S2Q01_a_201809|S2Q01_a_201510|S2Q01_a_201810|S2Q01_b_201511|S2Q01_b_201811|S2Q01_b_201512|S2Q01_b_201812|S2Q01_b_201513|S2Q01_b_201813|S2Q01_b_201514|S2Q01_b_201814|S2Q01_b_201515|S2Q01_b_201815|S2Q01_b_201516|S2Q01_b_201816|S2Q01_b_201517|S2Q01_b_201817|S2Q01_b_201518|S2Q01_b_201818|S2Q01_b_201519|S2Q01_b_201819|S2Q02_01|S2Q02_02|S2Q02_03|S2Q02_04|S2Q02_05|S2Q02_06|S2Q02_07|S2Q02_08|S2Q02_09|S2Q02_10|S2Q02_11|S3Q01_20151|S3Q01_20181|S3Q01_20152|S3Q01_20182|S3Q02_01|S3Q02_02|S3Q02_03|S3Q02_04|S3Q02_05|S3Q02_06|S3Q02_07|S3Q02_08|S3Q02_09|S3Q02_10|S3Q03-AL|S4Q01|           S4Q011_AL|S4Q021|S4Q022|S4Q023|             S4Q024|S4Q025|S4Q026|S4Q031|S4Q041|S4Q051|            S4Q061|            S4Q071|S5Q010-AL|S5Q02|S5Q03|             S5Q04|S5Q05_a01|S5Q05_a02|S5Q05_a03|S5Q05_a04|S5Q05_a05|S5Q05_a06|S5Q05_a24|S5Q05_a07|S5Q05_a08|S5Q05_a09|S5Q05_a10|S5Q05_a11|S5Q05_a12|S5Q05_b13|S5Q05_b14|S5Q05_b15|S5Q05_b16|S5Q05_b17|S5Q05_b18|S5Q05_b19|S5Q05_b20|S5Q05_b21|S5Q05_b22|            features|\n",
      "+--------+-----+---------+-------------------+---------+---------+---------+---------+--------------------+--------------------+------+------+-------------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+--------------------+------+------+------+-------------------+------+------+------+------+------+------------------+------------------+---------+-----+-----+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------------+\n",
      "|     0.0|    1|      2.0|           0.453125|      1.0|      1.0|      1.0|      1.0|0.005732484076433121| 0.00784313725490196|   0.9|   0.1| 0.7142857142857143|          0.25|          0.25|          0.25|          0.25|           0.25|          0.25|          0.25|          0.25|          0.25|           0.5|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|           0.5|           0.5|          0.25|          0.25|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|     0.0|     0.0|     0.0|    0.25|     0.0|     0.0|    0.25|     0.5|     0.0|     0.0|     0.0|       0.25|       0.25|       0.25|       0.25|     0.5|     0.5|    0.25|    0.75|    0.25|    0.25|    0.25|    0.25|    0.25|    0.25|     0.7|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0| 0.25|0.6666666666666666|      1.0|      1.0|      1.0|      1.0|      1.0|      0.5|      1.0|      1.0|      1.0|      0.5|      0.5|      0.5|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|[0.0,1.0,0.453125...|\n",
      "|     0.0|    0|      0.0|0.48333333333333334|      0.5|      0.5|      0.5|      0.0|0.006369426751592357|  0.0196078431372549|  0.87|  0.13| 0.5714285714285714|          0.75|          0.75|          0.75|          0.75|           0.75|          0.75|          0.25|          0.75|           0.0|           1.0|           0.5|           1.0|          0.75|          0.75|           0.0|          0.75|           0.0|          0.75|           0.0|           1.0|           0.0|          0.75|           0.0|           0.5|           0.0|          0.75|           0.0|          0.75|           0.0|          0.25|           0.0|          0.75|           0.0|          0.25|           0.0|           1.0|           0.0|          0.25|     1.0|     1.0|    0.75|     0.5|    0.75|     0.5|     1.0|     1.0|     1.0|     1.0|    0.75|       0.25|       0.75|       0.25|       0.75|     1.0|     1.0|    0.75|     1.0|     0.5|     1.0|     1.0|     0.5|    0.75|     1.0|     0.0|    0|              9.8E-5|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0| 0.75|0.3333333333333333|     0.75|      1.0|     0.75|      1.0|      1.0|     0.25|     0.75|      1.0|      1.0|      0.5|      0.5|      0.0|      0.5|      0.0|      0.0|      0.0|     0.75|      1.0|      0.5|     0.75|      1.0|     0.25|      0.5|[0.0,0.0,0.483333...|\n",
      "|     0.0|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001273885350318...| 0.00392156862745098|   0.7|   0.3| 0.2857142857142857|           0.5|           0.5|          0.25|          0.25|           0.25|          0.25|           0.5|           0.5|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|          0.75|          0.75|          0.25|          0.25|           0.0|           0.0|          0.25|          0.25|           0.0|           0.0|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|          0.25|          0.25|           0.0|           0.0|           0.5|           0.5|           0.0|           0.0|     0.0|     0.0|    0.25|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|        0.5|        0.5|        0.5|        0.5|    0.75|    0.75|     0.5|    0.75|    0.75|     0.5|     0.5|     0.5|     0.5|    0.75|     0.3|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|      0.0|      0.0|      0.0|      1.0|      1.0|      0.0|     0.25|      0.5|     0.25|     0.25|     0.25|      0.0|      1.0|     0.25|      0.5|     0.25|      0.5|     0.75|     0.25|     0.25|     0.75|      0.0|     0.25|[0.0,1.0,0.479166...|\n",
      "|     0.1|    1|      2.0| 0.4270833333333333|      1.0|      1.0|      1.0|      1.0|6.369426751592356E-4| 0.00196078431372549|   0.9|   0.1|                0.0|           0.0|          0.25|           0.0|          0.25|            0.0|          0.25|           0.0|          0.25|           0.0|          0.25|           0.0|          0.25|           0.5|          0.25|          0.25|          0.25|           0.0|          0.25|          0.25|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|    0.25|    0.25|    0.25|     0.5|    0.25|     0.5|    0.25|     0.5|     0.5|     0.5|    0.25|       0.25|        0.5|       0.25|        0.5|    0.25|    0.25|     0.5|     0.5|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0|  0.5|0.6666666666666666|     0.25|      0.0|      0.0|      0.0|      0.0|      0.0|      0.5|      0.5|      0.5|      0.5|     0.25|      0.0|      0.5|      0.5|      0.5|      0.5|     0.25|      0.5|      0.5|     0.25|     0.25|     0.25|      0.0|[0.1,1.0,0.427083...|\n",
      "|     0.0|    0|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001273885350318...| 0.00392156862745098|   0.7|   0.3| 0.2857142857142857|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.0|           0.0|     0.5|     0.5|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     0.0|        0.5|        0.5|        0.5|        0.5|     1.0|     1.0|     1.0|     1.0|     1.0|    0.75|    0.75|    0.75|    0.75|     1.0|     0.0|    1|            2.667E-5|   0.7|   0.3|   0.0|                0.0|   0.0|   0.0|   0.2| 0.125|   0.4|0.6666666666666666|0.3333333333333333|      0.0|    0| 0.25|0.6666666666666666|      0.0|      0.0|     0.25|      1.0|      1.0|      1.0|      1.0|      0.5|      0.5|     0.75|     0.25|      0.5|      1.0|      0.5|      1.0|      1.0|      0.5|      1.0|      1.0|      0.0|      1.0|      1.0|      0.0|[0.0,0.0,0.479166...|\n",
      "|     0.1|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|3.184713375796178E-4| 0.01019607843137255|  0.98|  0.02|                0.0|           0.5|           0.5|           0.5|           0.5|            0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|    0.25|    0.25|     0.5|    0.25|    0.25|    0.25|     0.5|     0.5|     0.5|    0.75|     1.0|        0.5|        0.5|        0.5|        0.5|     0.5|     1.0|     0.5|     1.0|     0.5|    0.75|     0.5|     0.5|    0.75|    0.75|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|     0.25|     0.25|      0.5|     0.75|      1.0|     0.75|      0.5|     0.75|      0.5|     0.25|      0.0|      0.0|      1.0|     0.25|      1.0|     0.75|      0.5|      0.5|     0.25|      0.5|      1.0|      1.0|      0.5|[0.1,1.0,0.479166...|\n",
      "|     0.0|    0|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.003821656050955414|                 0.0|  0.75|  0.25| 0.5714285714285714|          0.75|          0.75|          0.75|          0.75|           0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|           1.0|           1.0|          0.75|          0.75|           1.0|           1.0|           1.0|           1.0|          0.75|          0.75|          0.75|          0.75|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|    0.75|    0.75|    0.75|     1.0|     1.0|    0.75|     1.0|     1.0|    0.75|    0.75|     0.5|        0.5|        0.5|        0.5|        0.5|     1.0|    0.75|     1.0|     1.0|    0.75|    0.75|     1.0|    0.75|     0.5|     1.0|     0.2|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0|  0.0|0.6666666666666666|     0.75|     0.75|      1.0|      0.5|      0.5|      0.5|      1.0|      1.0|      1.0|      1.0|     0.75|     0.75|     0.75|     0.75|     0.75|     0.75|      0.5|     0.75|     0.75|      0.5|      0.5|      0.5|      0.5|[0.0,0.0,0.479166...|\n",
      "|     0.0|    1|      2.0| 0.4270833333333333|      0.5|      0.5|      0.5|      0.5|0.041401273885350316| 0.12254901960784313|   0.9|   0.1|                1.0|           0.5|           1.0|           0.5|          0.75|            0.5|           1.0|          0.25|           1.0|          0.25|          0.75|           1.0|           0.5|           0.0|           1.0|           0.0|           1.0|           0.0|          0.75|           0.0|           1.0|           0.0|           1.0|          0.25|          0.75|           1.0|           1.0|           0.5|          0.75|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|    0.75|    0.75|     0.5|    0.75|    0.75|     1.0|    0.75|     1.0|     0.5|     1.0|     1.0|        0.5|        0.5|        0.5|        0.5|    0.75|     1.0|     0.5|     0.5|     1.0|    0.75|    0.75|    0.75|    0.25|     0.5|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0|  0.5|0.6666666666666666|      0.0|     0.25|      0.5|      1.0|      1.0|      0.5|      0.5|      0.5|      0.5|     0.25|     0.25|      0.0|     0.75|      0.0|      0.5|      0.5|     0.75|      0.5|      0.0|      0.5|     0.25|      0.0|      0.0|[0.0,1.0,0.427083...|\n",
      "|     0.0|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|                 1.0|                 1.0|   0.7|   0.3| 0.5714285714285714|           1.0|           1.0|          0.75|           1.0|            1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           0.5|           1.0|          0.75|           1.0|           0.5|           1.0|           0.5|           1.0|           0.5|          0.75|           0.5|           1.0|           0.5|          0.75|           0.5|          0.75|          0.75|          0.75|           0.5|           0.5|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|          0.75|          0.75|    0.75|     0.5|     1.0|     1.0|     1.0|    0.75|     1.0|     1.0|     1.0|     1.0|     1.0|        0.5|       0.75|        0.5|       0.75|    0.75|    0.75|    0.75|     1.0|     1.0|     1.0|     1.0|    0.75|    0.75|    0.75|     0.2|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.2|    0|  0.0|0.6666666666666666|      0.5|      0.5|      0.5|     0.25|     0.75|      0.5|     0.75|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|     0.75|     0.75|     0.75|     0.75|     0.75|      0.5|     0.75|      0.5|[0.0,1.0,0.479166...|\n",
      "|     0.0|    1|      2.0| 0.2708333333333333|      1.0|      1.0|      1.0|      1.0|0.001910828025477707|0.005882352941176...|   0.4|   0.6|                0.0|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           0.0|           0.0|           1.0|           1.0|           1.0|           1.0|           0.0|           0.0|           1.0|           1.0|           0.0|           0.0|           1.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|     1.0|     0.0|     0.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|        0.5|        0.5|        0.5|        0.5|     1.0|     0.5|     1.0|     1.0|     0.5|     1.0|     1.0|     1.0|     1.0|     1.0|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    1| 0.25|0.6666666666666666|      0.5|      0.5|      0.5|      1.0|      1.0|      0.5|      0.5|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      1.0|      0.5|      1.0|      1.0|      0.5|      1.0|      1.0|      0.5|[0.0,1.0,0.270833...|\n",
      "|     0.0|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001910828025477707|0.004901960784313725|   0.9|   0.1|0.14285714285714285|          0.25|          0.25|          0.25|          0.25|           0.25|          0.25|          0.25|          0.25|           0.5|           0.5|          0.25|          0.25|          0.25|          0.75|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|     0.5|     0.5|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|        0.5|        0.5|        0.5|        0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      1.0|    0| 0.25|0.6666666666666666|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|[0.0,1.0,0.479166...|\n",
      "|     0.0|    0|      0.0|            0.53125|      0.5|      0.5|      0.0|      0.0|0.001592356687898...|0.004901960784313725|   0.8|   0.2|0.42857142857142855|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           1.0|           1.0|          0.25|          0.25|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           0.0|           0.0|          0.75|          0.75|           1.0|           1.0|          0.75|          0.75|           0.5|           0.5|          0.75|          0.75|          0.75|          0.75|           0.0|          0.25|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|    0.25|    0.25|     0.5|     0.5|    0.75|    0.75|     1.0|     1.0|    0.75|    0.75|     0.5|        0.5|        0.5|        0.5|        0.5|     1.0|     1.0|     0.5|    0.75|    0.75|    0.75|     1.0|     0.5|     0.5|     0.5|     0.0|    1|                 0.0|   0.3|   0.0|   0.2| 0.7142857142857143|   0.0|   0.0|   0.2|  0.25|   1.0|0.6666666666666666|0.3333333333333333|      0.0|    0| 0.25|0.6666666666666666|      0.0|      0.0|      0.0|     0.75|      1.0|      1.0|      1.0|      1.0|     0.75|     0.75|     0.75|      0.5|      1.0|      0.5|      0.5|      1.0|      1.0|      0.5|      0.5|     0.75|     0.75|     0.75|      0.5|[0.0,0.0,0.53125,...|\n",
      "|     1.0|    0|      1.0| 0.4791666666666667|      0.5|      0.0|      0.5|      1.0|0.007961783439490446|0.022549019607843137|  0.82|  0.18|                1.0|           0.5|           1.0|          0.25|          0.75|           0.25|          0.25|          0.25|           0.5|           0.0|           0.0|           0.5|           0.5|           0.0|           0.0|           0.5|           1.0|           0.0|           0.0|           0.0|           0.0|           0.5|           1.0|           0.0|           0.0|           0.0|           0.0|          0.25|           0.5|           0.5|           0.5|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|     0.5|     0.0|     0.5|     0.5|    0.75|    0.25|     0.5|     1.0|     0.5|     0.5|     0.5|        0.5|        0.5|        0.5|        0.5|     0.5|    0.75|     0.0|    0.75|    0.75|    0.75|     0.5|     0.5|     0.5|    0.25|     0.3|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|      0.0|      0.0|     0.75|     0.75|     0.75|      0.0|      0.5|     0.75|     0.75|     0.75|      0.5|      0.0|      0.5|      0.0|     0.75|      0.5|      0.5|      0.0|      0.0|     0.25|      0.5|      0.0|      0.0|(116,[0,2,3,5,6,7...|\n",
      "|     0.0|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001910828025477707|0.004901960784313725|   0.8|   0.2|0.14285714285714285|           0.5|           0.5|           0.5|           0.5|           0.25|           0.5|          0.25|          0.25|           0.5|          0.25|          0.25|           0.5|          0.75|           0.5|           0.5|          0.75|           0.5|           0.5|          0.25|           0.5|          0.25|           0.5|           0.5|          0.75|          0.25|           0.5|           0.5|          0.25|          0.25|           0.5|           0.5|          0.75|          0.25|           0.5|           0.5|          0.25|           0.5|          0.25|     0.5|    0.75|    0.25|     0.5|    0.25|     0.5|    0.25|    0.25|     0.5|    0.25|     0.5|        0.5|        0.5|        0.5|        0.5|     0.5|    0.75|     0.5|    0.75|    0.75|     0.5|     0.5|     0.5|    0.75|    0.75|     0.0|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0|  0.5|0.6666666666666666|     0.25|     0.25|     0.25|      0.5|     0.75|     0.25|     0.75|      1.0|     0.75|      1.0|      0.5|      0.5|     0.75|      0.5|     0.75|     0.25|     0.25|     0.75|      0.5|     0.25|      1.0|      1.0|     0.75|[0.0,1.0,0.479166...|\n",
      "|     0.0|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      1.0|      0.5|0.001592356687898...| 0.00392156862745098|   0.4|   0.6|0.14285714285714285|           0.5|           0.5|           0.5|           0.5|            0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|           0.5|          0.75|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|           0.5|          0.25|          0.25|          0.25|          0.25|    0.25|    0.25|     0.5|     0.5|     0.5|     0.5|     0.5|    0.75|    0.75|     1.0|     1.0|       0.25|        0.5|       0.25|        0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.5|     0.0|    1|              4.0E-4|   0.8|  0.07|  0.03|0.07142857142857142|  0.05|   0.0|   0.2| 0.125|   0.2|0.1111111111111111|0.3333333333333333|      0.0|    0|  0.5|0.6666666666666666|      0.5|      0.5|      0.5|      0.5|      0.5|      0.0|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|     0.75|     0.75|     0.75|      0.5|      0.5|      0.5|      0.5|      0.5|     0.25|     0.25|[0.0,1.0,0.479166...|\n",
      "|     0.0|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5|0.001273885350318...| 9.80392156862745E-4|   0.7|   0.3|0.14285714285714285|           0.5|           0.5|           0.5|           0.5|           0.75|          0.75|          0.75|          0.75|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|          0.75|          0.75|          0.25|          0.25|     1.0|    0.75|    0.75|    0.75|    0.75|    0.25|     0.5|    0.75|    0.75|    0.75|    0.75|        0.5|        0.5|       0.25|       0.25|     0.5|     0.0|     0.5|    0.75|    0.75|    0.75|    0.75|    0.75|    0.75|    0.75|     0.1|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.0|    0| 0.75|0.6666666666666666|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|     0.75|     0.75|     0.75|     0.75|      0.5|     0.75|     0.75|      1.0|      1.0|      1.0|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|      0.5|[0.0,1.0,0.479166...|\n",
      "|     0.0|    1|      2.0| 0.4270833333333333|      1.0|      0.5|      0.5|      1.0|9.554140127388535E-4| 9.80392156862745E-4|   0.8|   0.2|0.14285714285714285|          0.25|          0.25|          0.25|          0.25|            0.5|           0.5|          0.25|          0.25|          0.25|           0.5|          0.25|           0.5|          0.25|          0.25|          0.25|          0.25|          0.25|          0.25|           0.0|          0.25|           0.0|          0.25|           0.0|          0.25|          0.25|           0.5|          0.25|           0.5|          0.25|          0.25|           0.0|           0.0|           0.0|           0.0|          0.25|          0.25|          0.25|          0.25|     0.0|     0.0|     0.5|     0.5|     0.5|     0.5|    0.75|    0.75|    0.25|    0.75|    0.75|        0.5|       0.25|        0.5|       0.25|     0.5|    0.75|    0.25|    0.75|    0.25|     0.5|     0.5|    0.25|    0.25|    0.75|     0.1|    1|0.006666666666666667|   0.0|   0.5|   0.5|                0.0|   0.0|   0.0|   0.8| 0.125|   0.4|0.5555555555555556|0.3333333333333333|      0.0|    0|  0.0|0.6666666666666666|     0.25|     0.25|      0.5|      0.5|     0.75|      0.5|     0.75|     0.75|     0.75|     0.75|      0.5|      0.5|     0.75|     0.25|     0.75|      0.5|     0.25|     0.25|     0.25|     0.25|     0.25|     0.25|     0.25|[0.0,1.0,0.427083...|\n",
      "|     0.0|    0|      2.0| 0.4270833333333333|      1.0|      0.5|      0.5|      1.0|0.005414012738853503|0.014705882352941176|   0.5|   0.5| 0.7142857142857143|           1.0|           1.0|           1.0|           1.0|            1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|          0.75|           1.0|           1.0|           1.0|          0.75|           1.0|          0.75|           1.0|          0.75|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|     1.0|    0.75|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|    0.75|     1.0|    0.75|        0.5|       0.75|        0.5|       0.75|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     0.1|    1|                0.02|   0.7|   0.0|   0.1|0.14285714285714285|   0.1|   0.0|   0.2|  0.25|   0.2|0.6666666666666666|0.3333333333333333|      0.3|    1| 0.25|0.6666666666666666|      0.5|      0.5|     0.75|     0.75|      1.0|      1.0|      1.0|      1.0|     0.75|     0.75|     0.75|      0.5|     0.75|      1.0|      1.0|      1.0|      1.0|     0.75|      0.5|      0.5|     0.75|      0.5|      0.5|[0.0,0.0,0.427083...|\n",
      "|     0.1|    1|      1.0| 0.4791666666666667|      0.5|      0.5|      0.5|      0.5| 0.00678343949044586| 0.01019607843137255|   0.6|   0.4|                0.0|           0.5|           0.5|           0.5|          0.25|            0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|           0.5|          0.75|          0.75|          0.75|          0.75|           0.5|           0.5|          0.75|           0.5|           0.5|           0.5|          0.25|          0.25|           0.0|           0.0|          0.25|          0.25|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|    0.75|    0.25|    0.25|     0.5|     0.5|     0.5|     0.5|     1.0|    0.25|     1.0|     1.0|        0.5|        0.0|        0.0|        0.0|     0.5|    0.75|     0.5|    0.75|    0.75|    0.75|     0.5|    0.25|    0.25|    0.25|     0.3|    0|                 0.0|   0.0|   0.0|   0.0|                0.0|   0.0|   0.0|   0.0|   0.0|   0.0|               0.0|               0.0|      0.2|    0| 0.25|0.6666666666666666|     0.25|      0.5|      0.0|      1.0|      1.0|     0.25|     0.75|     0.75|      0.0|      0.0|     0.75|     0.75|     0.75|      1.0|      1.0|      1.0|     0.75|      0.5|      0.5|      0.5|      0.5|      0.5|      0.0|[0.1,1.0,0.479166...|\n",
      "|     0.0|    0|      0.0|            0.53125|      0.5|      0.5|      0.5|      0.0|0.007961783439490446| 0.00980392156862745|  0.72|  0.28|                1.0|          0.75|           1.0|          0.75|           1.0|            1.0|           1.0|           1.0|           1.0|          0.75|           1.0|          0.75|           1.0|           0.5|           1.0|          0.75|           1.0|          0.75|           1.0|          0.75|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|           1.0|          0.75|           0.5|          0.75|           1.0|          0.75|           1.0|          0.75|           1.0|          0.75|          0.75|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|        0.5|        1.0|        0.5|        1.0|    0.75|    0.75|    0.75|     1.0|     1.0|     1.0|     1.0|     1.0|    0.75|     1.0|     0.0|    1|                 0.1|   0.5|   0.2|   0.3|                0.0|   0.0|   0.0|   0.2| 0.125|   0.2|0.5555555555555556|0.3333333333333333|      0.0|    0| 0.75|               1.0|     0.25|      0.5|      0.5|      1.0|      1.0|     0.75|     0.75|     0.75|     0.75|     0.75|      0.5|      1.0|      1.0|      1.0|      0.5|     0.75|      0.5|      0.5|      0.5|     0.75|      0.5|      0.5|      0.0|[0.0,0.0,0.53125,...|\n",
      "+--------+-----+---------+-------------------+---------+---------+---------+---------+--------------------+--------------------+------+------+-------------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----------+-----------+-----------+-----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-----+--------------------+------+------+------+-------------------+------+------+------+------+------+------------------+------------------+---------+-----+-----+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a decision tree classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "def calculate_multiclass_metrics(confusion_matrix):\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    FP = np.sum(confusion_matrix, axis=0) - TP\n",
    "    FN = np.sum(confusion_matrix, axis=1) - TP\n",
    "    TN = np.sum(confusion_matrix) - (FP + FN + TP)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.sum(TP) / np.sum(confusion_matrix)\n",
    "\n",
    "    # Precision, Recall, F1-Score for each class\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Macro-averaged metrics\n",
    "    precision_macro = np.nanmean(precision)\n",
    "    recall_macro = np.nanmean(recall)\n",
    "    f1_score_macro = np.nanmean(f1_score)\n",
    "\n",
    "    return accuracy, precision_macro, recall_macro, f1_score_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(spark_df, target):\n",
    "    (train_data, test_data) = spark_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "    dt = DecisionTreeClassifier(labelCol=target, featuresCol=\"features\")\n",
    "\n",
    "    model = dt.fit(train_data)\n",
    "\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Collect labels and predictions\n",
    "    labels_and_preds = predictions.select(target, \"prediction\").collect()\n",
    "    labels = [row[target] for row in labels_and_preds]\n",
    "    preds = [row['prediction'] for row in labels_and_preds]\n",
    "    labels = np.array(labels)\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    classes = np.unique(labels)\n",
    "    confusion_matrix = np.zeros((len(classes), len(classes)))\n",
    "    for i, j in zip(labels, preds):\n",
    "        confusion_matrix[int(i)][int(j)] += 1\n",
    "    \n",
    "    # metrics are calculated using their mathematical formulas as discussed in the paper.\n",
    "    accuracy, precision_macro, recall_macro, f1_score_macro = calculate_multiclass_metrics(confusion_matrix)\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision_macro,\n",
    "        \"recall\": recall_macro,\n",
    "        \"f1\": f1_score_macro\n",
    "    }\n",
    "    print(f\"Evaluation metrics for {target}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {round(value, 4)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for S1Q061nor:\n",
      "Accuracy: 0.9884\n",
      "Precision: 0.9937\n",
      "Recall: 0.9722\n",
      "F1: 0.9823\n"
     ]
    }
   ],
   "source": [
    "model = train_and_evaluate(preprocessed_df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify features' importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = assembler.getInputCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, target):\n",
    "    importances = model.featureImportances\n",
    "    indices = np.argsort(importances)[::-1].tolist()  # Convert indices to a list of integers\n",
    "\n",
    "    filtered_indices = [idx for idx in indices if importances[idx] > 0.01]\n",
    "    filtered_importances = [importances[idx] for idx in filtered_indices]\n",
    "    filtered_feature_names = [feature_names[idx] for idx in filtered_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 20)) \n",
    "    plt.title(f\"Feature importances for {target}\")\n",
    "    plt.barh(range(len(filtered_importances)), filtered_importances, align=\"center\")\n",
    "    plt.yticks(range(len(filtered_importances)), filtered_feature_names, rotation=0)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature_importance(model, feature_names, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(model, feature_names, threshold=0.01):\n",
    "    importances = model.featureImportances\n",
    "    selected_features = [feature_names[idx] for idx in range(len(importances)) if importances[idx] > threshold]\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['S1Q06_P1nor']\n"
     ]
    }
   ],
   "source": [
    "selected_features = select_important_features(model, feature_names)\n",
    "print(f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model only with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_with_selected_features(spark_df, selected_features, target):\n",
    "    assembler = VectorAssembler(inputCols=selected_features, outputCol=\"selected_features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + [assembler])\n",
    "    pipeline_model = pipeline.fit(spark_df)\n",
    "    preprocessed_df = pipeline_model.transform(spark_df)\n",
    "\n",
    "    (train_data, test_data) = preprocessed_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "    dt = DecisionTreeClassifier(labelCol=target, featuresCol=\"selected_features\")\n",
    "\n",
    "    model = dt.fit(train_data)\n",
    "\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Collect labels and predictions\n",
    "    labels_and_preds = predictions.select(target, \"prediction\").collect()\n",
    "    labels = [row[target] for row in labels_and_preds]\n",
    "    preds = [row['prediction'] for row in labels_and_preds]\n",
    "    labels = np.array(labels)\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    classes = np.unique(labels)\n",
    "    confusion_matrix = np.zeros((len(classes), len(classes)))\n",
    "    for i, j in zip(labels, preds):\n",
    "        confusion_matrix[int(i)][int(j)] += 1\n",
    "    \n",
    "    # metrics are calculated using their mathematical formulas as discussed in the paper.\n",
    "    accuracy, precision_macro, recall_macro, f1_score_macro = calculate_multiclass_metrics(confusion_matrix)\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision_macro,\n",
    "        \"recall\": recall_macro,\n",
    "        \"f1\": f1_score_macro\n",
    "    }\n",
    "\n",
    "    print(f\"Evaluation metrics for {target}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {round(value, 4)}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for S1Q061nor:\n",
      "Accuracy: 0.9884\n",
      "Precision: 0.9937\n",
      "Recall: 0.9722\n",
      "F1: 0.9823\n"
     ]
    }
   ],
   "source": [
    "selected_model = retrain_with_selected_features(spark_df, selected_features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We can see that indeed following this new approach we get different results for the metrics discussed in class for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefine all the previously implemented metrics in Exercise 3 (Categorical Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we will present the results in table format at the end of the exercise after compiling the results of all the different runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Excel files containing before and after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpp_df = pd.read_excel('Before_Pre_Processing.xlsx')\n",
    "app_df = pd.read_excel('After_Pre_Processing.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to generate multiple iterations of decision trees and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "def calculate_multiclass_metrics(confusion_matrix):\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    FP = np.sum(confusion_matrix, axis=0) - TP\n",
    "    FN = np.sum(confusion_matrix, axis=1) - TP\n",
    "    TN = np.sum(confusion_matrix) - (FP + FN + TP)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.sum(TP) / np.sum(confusion_matrix)\n",
    "\n",
    "    # Precision, Recall, F1-Score for each class\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Macro-averaged metrics\n",
    "    precision_macro = np.nanmean(precision)\n",
    "    recall_macro = np.nanmean(recall)\n",
    "    f1_score_macro = np.nanmean(f1_score)\n",
    "\n",
    "    return accuracy, precision_macro, recall_macro, f1_score_macro\n",
    "\n",
    "def generate_decision_trees(X, y):\n",
    "    accuracy, precision, recall, f1_score  = [], [], [], []\n",
    "\n",
    "    for _ in range(30):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        dt_classifier = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "        predictions = dt_classifier.predict(X_test)\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        classes = np.unique(y)\n",
    "        confusion_matrix = np.zeros((len(classes), len(classes)))\n",
    "        \n",
    "        for i, j in zip(y_test, predictions):\n",
    "            confusion_matrix[int(i)][int(j)] += 1\n",
    "\n",
    "        # metrics are calculated using their mathematical formulas as discussed in the paper.\n",
    "        acc, prec, rec, f1 = calculate_multiclass_metrics(confusion_matrix)\n",
    "        \n",
    "        accuracy.append(acc)\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        f1_score.append(f1)\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "def print_results(accuracy, precision, recall, f1_score):\n",
    "    print('Accuracy')\n",
    "    print('Min: ', min(accuracy))\n",
    "    print('Max: ', max(accuracy))\n",
    "    print('Avg: ', np.mean(accuracy))\n",
    "    print('Std: ', np.std(accuracy), '\\n')\n",
    "\n",
    "    print('Precision')\n",
    "    print('Min: ', min(precision))\n",
    "    print('Max: ', max(precision))\n",
    "    print('Avg: ', np.mean(precision))\n",
    "    print('Std: ', np.std(precision), '\\n')\n",
    "\n",
    "    print('Recall')\n",
    "    print('Min: ', min(recall))\n",
    "    print('Max: ', max(recall))\n",
    "    print('Avg: ', np.mean(recall))\n",
    "    print('Std: ', np.std(recall), '\\n')\n",
    "\n",
    "    print('F1 Score')\n",
    "    print('Min: ', min(f1_score))\n",
    "    print('Max: ', max(f1_score))\n",
    "    print('Avg: ', np.mean(f1_score))\n",
    "    print('Std: ', np.std(f1_score), '\\n')\n",
    "\n",
    "\n",
    "def create_df(predictor_set, dataset, target, accuracy, precision, recall, f1_score):\n",
    "\n",
    "    lst = [\n",
    "        [predictor_set, dataset, target, \"Accuracy\", min(accuracy), max(accuracy), np.mean(accuracy), np.std(accuracy)], \n",
    "        [predictor_set, dataset, target, \"Precision\", min(precision), max(precision), np.mean(precision), np.std(precision)], \n",
    "        [predictor_set, dataset, target, \"Recall\", min(recall), max(recall), np.mean(recall), np.std(recall)],\n",
    "        [predictor_set, dataset, target, \"F1-Score\", min(f1_score), max(f1_score), np.mean(f1_score), np.std(f1_score)],\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(lst, columns=['predictor_set', 'dataset', 'target', 'metric', 'min', 'max', 'avg', 'stddev'])\n",
    "    return df\n",
    "\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before-Pre-Processing (b), in which S1Q061 and S1Q06P1 are NOT pre-processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1Q061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.9814814814814815\n",
      "Max:  1.0\n",
      "Avg:  0.9950617283950616\n",
      "Std:  0.008189197013223196 \n",
      "\n",
      "Precision\n",
      "Min:  0.9888888888888889\n",
      "Max:  1.0\n",
      "Avg:  0.9973493805214235\n",
      "Std:  0.004414806937844033 \n",
      "\n",
      "Recall\n",
      "Min:  0.9444444444444445\n",
      "Max:  1.0\n",
      "Avg:  0.9890692640692641\n",
      "Std:  0.01862246567801867 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.9640472521828455\n",
      "Max:  1.0\n",
      "Avg:  0.9928145409211139\n",
      "Std:  0.012190120681441152 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All features\n",
    "# Since the data is continuous we have decided to bin the data into 3 separate categories\n",
    "bpp_df['S1Q061_cat'] = pd.cut(bpp_df['S1Q061'], bins=3, labels=False)\n",
    "\n",
    "all_cols = bpp_df.columns.drop(['USER', 'S1Q061', 'S1Q061_cat'])\n",
    "X, y = bpp_df[all_cols].values, bpp_df['S1Q061_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"ALL\", \"b\", 'S1Q061', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.3148148148148148\n",
      "Max:  0.6111111111111112\n",
      "Avg:  0.4623456790123456\n",
      "Std:  0.06936964421477877 \n",
      "\n",
      "Precision\n",
      "Min:  0.2566583953680728\n",
      "Max:  0.45137395459976104\n",
      "Avg:  0.3439604607446538\n",
      "Std:  0.058819617328342406 \n",
      "\n",
      "Recall\n",
      "Min:  0.20614035087719298\n",
      "Max:  0.5237352737352737\n",
      "Avg:  0.3388267737981651\n",
      "Std:  0.07507352074236912 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.25460030165912517\n",
      "Max:  0.4951923076923077\n",
      "Avg:  0.38980487045385986\n",
      "Std:  0.059766346000592406 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVBOOL features\n",
    "invbool_cols = ['S2Q01_a_201801', 'S1Q05', 'S5Q05_a01', 'S5Q05_a02', 'S5Q05_a03', 'S5Q05_a04', 'S5Q05_a05',\t'S5Q05_a06', 'S5Q05_a24', \n",
    "                'S5Q05_a07', 'S5Q05_a08', 'S5Q05_a09', 'S5Q05_a10', 'S5Q05_a11', 'S5Q05_a12', 'S5Q05_b13', 'S5Q05_b14', 'S5Q05_b15',\n",
    "                'S5Q05_b16', 'S5Q05_b17', 'S5Q05_b18', 'S5Q05_b19', 'S5Q05_b20', 'S5Q05_b21',\t'S5Q05_b22', 'S4Q011_AL']\n",
    "X, y = bpp_df[invbool_cols].values, bpp_df['S1Q061_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVBOOL\", \"b\", 'S1Q061', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.3333333333333333\n",
      "Max:  0.6111111111111112\n",
      "Avg:  0.47037037037037027\n",
      "Std:  0.06226923561935059 \n",
      "\n",
      "Precision\n",
      "Min:  0.24052287581699347\n",
      "Max:  0.4777777777777778\n",
      "Avg:  0.3535725298864335\n",
      "Std:  0.05800010719015674 \n",
      "\n",
      "Recall\n",
      "Min:  0.23942652329749103\n",
      "Max:  0.5198412698412699\n",
      "Avg:  0.3572828497886698\n",
      "Std:  0.06550811644065724 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.28314176245210726\n",
      "Max:  0.6111111111111112\n",
      "Avg:  0.406435665669882\n",
      "Std:  0.08066012358557886 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVCONT features\n",
    "invcont_cols = ['S2Q01_a_201801', 'S1Q05', 'S5Q05_a01', 'S5Q05_a02', 'S5Q05_a03', 'S5Q05_a04', 'S5Q05_a05',\t'S5Q05_a06', 'S5Q05_a24', \n",
    "                'S5Q05_a07', 'S5Q05_a08', 'S5Q05_a09', 'S5Q05_a10', 'S5Q05_a11', 'S5Q05_a12', 'S5Q05_b13', 'S5Q05_b14', 'S5Q05_b15',\n",
    "                'S5Q05_b16', 'S5Q05_b17', 'S5Q05_b18', 'S5Q05_b19', 'S5Q05_b20', 'S5Q05_b21',\t'S5Q05_b22', 'S4Q021', 'S4Q022', 'S4Q023',\n",
    "                'S4Q024', 'S4Q025', 'S4Q026']\n",
    "X, y = bpp_df[invcont_cols].values, bpp_df['S1Q061_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVCONT\", \"b\", 'S1Q061', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1Q06P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8333333333333334\n",
      "Max:  0.9814814814814815\n",
      "Avg:  0.9209876543209877\n",
      "Std:  0.03762901396081028 \n",
      "\n",
      "Precision\n",
      "Min:  0.3066666666666667\n",
      "Max:  0.9807692307692308\n",
      "Avg:  0.5392043503581081\n",
      "Std:  0.21584608832546373 \n",
      "\n",
      "Recall\n",
      "Min:  0.29411764705882354\n",
      "Max:  0.9871794871794872\n",
      "Avg:  0.5212835927791534\n",
      "Std:  0.21825953399322148 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.6164983164983164\n",
      "Max:  0.9906542056074767\n",
      "Avg:  0.9007348886926607\n",
      "Std:  0.10670354074795338 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All features\n",
    "# Since the data is continuous we have decided to bin the data into 3 separate categories\n",
    "bpp_df['S1Q06_P1_cat'] = pd.cut(bpp_df['S1Q06_P1'], bins=3, labels=False)\n",
    "\n",
    "all_cols = bpp_df.columns.drop(['USER', 'S1Q061_cat', 'S1Q06_P1', 'S1Q06_P1_cat'])\n",
    "X, y = bpp_df[all_cols].values, bpp_df['S1Q06_P1_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"ALL\", \"b\", 'S1Q06_P1', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8148148148148148\n",
      "Max:  0.9814814814814815\n",
      "Avg:  0.9018518518518517\n",
      "Std:  0.04061428185085428 \n",
      "\n",
      "Precision\n",
      "Min:  0.3071895424836601\n",
      "Max:  0.9814814814814815\n",
      "Avg:  0.4622892461000349\n",
      "Std:  0.19934758405379233 \n",
      "\n",
      "Recall\n",
      "Min:  0.3071895424836601\n",
      "Max:  0.9629629629629629\n",
      "Avg:  0.5070490428929729\n",
      "Std:  0.21342449844822808 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.6646464646464647\n",
      "Max:  0.9906542056074767\n",
      "Avg:  0.9088781838069833\n",
      "Std:  0.09323119547158652 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVBOOL features\n",
    "X, y = bpp_df[invbool_cols].values, bpp_df['S1Q06_P1_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVBOOL\", \"b\", 'S1Q06_P1', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8333333333333334\n",
      "Max:  0.9629629629629629\n",
      "Avg:  0.895679012345679\n",
      "Std:  0.03430788452244506 \n",
      "\n",
      "Precision\n",
      "Min:  0.3\n",
      "Max:  0.9629629629629629\n",
      "Avg:  0.43620678552066483\n",
      "Std:  0.18614877840496372 \n",
      "\n",
      "Recall\n",
      "Min:  0.29411764705882354\n",
      "Max:  0.9259259259259259\n",
      "Avg:  0.42647462044585127\n",
      "Std:  0.15572035023949388 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.6075036075036075\n",
      "Max:  0.9811320754716981\n",
      "Avg:  0.9344784763887618\n",
      "Std:  0.0638578876714464 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVCONT features\n",
    "X, y = bpp_df[invcont_cols].values, bpp_df['S1Q06_P1_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVCONT\", \"b\", 'S1Q06_P1', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After-Pre-Processing (anotp), in which S1Q061 and S1Q06P1 are NOT PRE-PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "anotp_df = app_df.drop(columns=['S1Q061nor', 'S1Q06_P1nor'])\n",
    "anotp_df = pd.merge(anotp_df, bpp_df[['USER', 'S1Q061', 'S1Q06_P1']], on='USER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1Q061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.9629629629629629\n",
      "Max:  1.0\n",
      "Avg:  0.9907407407407407\n",
      "Std:  0.012422599874998838 \n",
      "\n",
      "Precision\n",
      "Min:  0.980952380952381\n",
      "Max:  1.0\n",
      "Avg:  0.995198224436444\n",
      "Std:  0.006316670473867409 \n",
      "\n",
      "Recall\n",
      "Min:  0.9125\n",
      "Max:  1.0\n",
      "Avg:  0.9827443081609748\n",
      "Std:  0.024914002742771477 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.9424063532222925\n",
      "Max:  1.0\n",
      "Avg:  0.9883423108781303\n",
      "Std:  0.016450099968628968 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All features\n",
    "# Since the data is continuous we have decided to bin the data into 3 separate categories\n",
    "anotp_df['S1Q061_cat'] = pd.cut(anotp_df['S1Q061'], bins=3, labels=False)\n",
    "\n",
    "all_cols = anotp_df.columns.drop(['USER', 'S1Q061', 'S1Q061_cat'])\n",
    "X, y = anotp_df[all_cols].values, anotp_df['S1Q061_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"ALL\", \"anotp\", 'S1Q061', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.35185185185185186\n",
      "Max:  0.6111111111111112\n",
      "Avg:  0.4870370370370371\n",
      "Std:  0.0691495797559885 \n",
      "\n",
      "Precision\n",
      "Min:  0.21428571428571427\n",
      "Max:  0.5397707231040564\n",
      "Avg:  0.3695552946216303\n",
      "Std:  0.07281266505602393 \n",
      "\n",
      "Recall\n",
      "Min:  0.21125541125541125\n",
      "Max:  0.5577813390313391\n",
      "Avg:  0.3677103202633497\n",
      "Std:  0.07561770591137311 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.2747668997668998\n",
      "Max:  0.544536610343062\n",
      "Avg:  0.4048817341440738\n",
      "Std:  0.06964211142001611 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVBOOL features\n",
    "invbool_cols = ['S2Q01_a_201801', 'S1Q05', 'S5Q05_a01', 'S5Q05_a02', 'S5Q05_a03', 'S5Q05_a04', 'S5Q05_a05', 'S5Q05_a06', 'S5Q05_a24',\n",
    "                'S5Q05_a07', 'S5Q05_a08', 'S5Q05_a09', 'S5Q05_a10', 'S5Q05_a11', 'S5Q05_a12', 'S5Q05_b13', 'S5Q05_b14', 'S5Q05_b15',\n",
    "                'S5Q05_b16', 'S5Q05_b17', 'S5Q05_b18', 'S5Q05_b19', 'S5Q05_b20', 'S5Q05_b21', 'S5Q05_b22', 'S4Q011_AL']\n",
    "X, y = anotp_df[invbool_cols].values, anotp_df['S1Q061_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVBOOL\", \"anotp\", 'S1Q061', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.3333333333333333\n",
      "Max:  0.6296296296296297\n",
      "Avg:  0.4697530864197531\n",
      "Std:  0.06668095411602097 \n",
      "\n",
      "Precision\n",
      "Min:  0.25980392156862747\n",
      "Max:  0.5136867636867637\n",
      "Avg:  0.35051761096569234\n",
      "Std:  0.0619937547993416 \n",
      "\n",
      "Recall\n",
      "Min:  0.23665223665223664\n",
      "Max:  0.4826388888888889\n",
      "Avg:  0.3532264995297694\n",
      "Std:  0.056384635198068106 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.2946127946127946\n",
      "Max:  0.5533333333333333\n",
      "Avg:  0.3933527172353164\n",
      "Std:  0.05359977442998735 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVCONT features\n",
    "invcont_cols = ['S2Q01_a_201801', 'S1Q05', 'S5Q05_a01', 'S5Q05_a02', 'S5Q05_a03', 'S5Q05_a04', 'S5Q05_a05', 'S5Q05_a06', 'S5Q05_a24',\n",
    "                'S5Q05_a07', 'S5Q05_a08', 'S5Q05_a09', 'S5Q05_a10', 'S5Q05_a11', 'S5Q05_a12', 'S5Q05_b13', 'S5Q05_b14', 'S5Q05_b15',\n",
    "                'S5Q05_b16', 'S5Q05_b17', 'S5Q05_b18', 'S5Q05_b19', 'S5Q05_b20', 'S5Q05_b21', 'S5Q05_b22', 'S4Q021', 'S4Q022', 'S4Q023',\n",
    "                'S4Q024', 'S4Q025', 'S4Q026']\n",
    "X, y = anotp_df[invcont_cols].values, anotp_df['S1Q061_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVCONT\", \"anotp\", 'S1Q061', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1Q06P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8148148148148148\n",
      "Max:  1.0\n",
      "Avg:  0.9234567901234567\n",
      "Std:  0.037871263333772755 \n",
      "\n",
      "Precision\n",
      "Min:  0.3133333333333333\n",
      "Max:  1.0\n",
      "Avg:  0.5724992183402414\n",
      "Std:  0.21182807248131968 \n",
      "\n",
      "Recall\n",
      "Min:  0.3071895424836601\n",
      "Max:  1.0\n",
      "Avg:  0.5763011695674346\n",
      "Std:  0.21481766236755828 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.6181046676096181\n",
      "Max:  1.0\n",
      "Avg:  0.8570190361795172\n",
      "Std:  0.11600389576862036 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All features\n",
    "# Since the data is continuous we have decided to bin the data into 3 separate categories\n",
    "anotp_df['S1Q06_P1_cat'] = pd.cut(anotp_df['S1Q06_P1'], bins=3, labels=False)\n",
    "\n",
    "all_cols = anotp_df.columns.drop(['USER', 'S1Q061_cat', 'S1Q06_P1', 'S1Q06_P1_cat'])\n",
    "X, y = anotp_df[all_cols].values, anotp_df['S1Q06_P1_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"ALL\", \"anotp\", 'S1Q06_P1_cat', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.7962962962962963\n",
      "Max:  0.9629629629629629\n",
      "Avg:  0.8870370370370372\n",
      "Std:  0.042860136634172714 \n",
      "\n",
      "Precision\n",
      "Min:  0.2986111111111111\n",
      "Max:  0.9629629629629629\n",
      "Avg:  0.41225965483632554\n",
      "Std:  0.16316326670389264 \n",
      "\n",
      "Recall\n",
      "Min:  0.28758169934640526\n",
      "Max:  0.9074074074074074\n",
      "Avg:  0.4081442018940566\n",
      "Std:  0.15157883804932962 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.8865979381443299\n",
      "Max:  0.9811320754716981\n",
      "Avg:  0.9408287014903834\n",
      "Std:  0.02421459146030032 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVBOOL features\n",
    "X, y = anotp_df[invbool_cols].values, anotp_df['S1Q06_P1_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVBOOL\", \"anotp\", 'S1Q06_P1_cat', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8148148148148148\n",
      "Max:  0.9444444444444444\n",
      "Avg:  0.8962962962962961\n",
      "Std:  0.0329886153478701 \n",
      "\n",
      "Precision\n",
      "Min:  0.3125\n",
      "Max:  0.9444444444444444\n",
      "Avg:  0.43321546166711594\n",
      "Std:  0.14737991729950975 \n",
      "\n",
      "Recall\n",
      "Min:  0.29411764705882354\n",
      "Max:  0.9444444444444444\n",
      "Avg:  0.4279831664967229\n",
      "Std:  0.13163842620109112 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.6145833333333333\n",
      "Max:  0.9714285714285714\n",
      "Avg:  0.9185040282022932\n",
      "Std:  0.09027515335389467 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVCONT features\n",
    "X, y = anotp_df[invcont_cols].values, anotp_df['S1Q06_P1_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVCONT\", \"anotp\", 'S1Q06_P1_cat', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After-Pre-Processing (ap), in which S1Q061 and S1Q06P1 are PRE-PROCESSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1Q061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.9629629629629629\n",
      "Max:  1.0\n",
      "Avg:  0.9919753086419754\n",
      "Std:  0.010347564576691481 \n",
      "\n",
      "Precision\n",
      "Min:  0.981981981981982\n",
      "Max:  1.0\n",
      "Avg:  0.9958676993532307\n",
      "Std:  0.00528946457764107 \n",
      "\n",
      "Recall\n",
      "Min:  0.9333333333333332\n",
      "Max:  1.0\n",
      "Avg:  0.9849477682811015\n",
      "Std:  0.020713122093424394 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.9581320450885668\n",
      "Max:  1.0\n",
      "Avg:  0.9898415859310296\n",
      "Std:  0.013597678737855714 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All features\n",
    "# Since the data is continuous we have decided to bin the data into 3 separate categories\n",
    "app_df['S1Q061nor_cat'] = pd.cut(app_df['S1Q061nor'], bins=3, labels=False)\n",
    "\n",
    "all_cols = app_df.columns.drop(['USER', 'S1Q061nor', 'S1Q061nor_cat'])\n",
    "X, y = app_df[all_cols].values, app_df['S1Q061nor_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"ALL\", \"ap\", 'S1Q061nor', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.37037037037037035\n",
      "Max:  0.6111111111111112\n",
      "Avg:  0.4580246913580247\n",
      "Std:  0.057958985650891366 \n",
      "\n",
      "Precision\n",
      "Min:  0.22522522522522523\n",
      "Max:  0.43968531468531474\n",
      "Avg:  0.3507190500908823\n",
      "Std:  0.05735527156146479 \n",
      "\n",
      "Recall\n",
      "Min:  0.2450980392156863\n",
      "Max:  0.4505633255633256\n",
      "Avg:  0.35055826741440277\n",
      "Std:  0.05990267378225379 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.2805086192182967\n",
      "Max:  0.7042253521126761\n",
      "Avg:  0.41341653298976777\n",
      "Std:  0.09012747158899788 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVBOOL features\n",
    "invbool_cols = ['S2Q01_a_201801', 'S1Q05', 'S5Q05_a01', 'S5Q05_a02', 'S5Q05_a03', 'S5Q05_a04', 'S5Q05_a05', 'S5Q05_a06', 'S5Q05_a24',\n",
    "                'S5Q05_a07', 'S5Q05_a08', 'S5Q05_a09', 'S5Q05_a10', 'S5Q05_a11', 'S5Q05_a12', 'S5Q05_b13', 'S5Q05_b14', 'S5Q05_b15',\n",
    "                'S5Q05_b16', 'S5Q05_b17', 'S5Q05_b18', 'S5Q05_b19', 'S5Q05_b20', 'S5Q05_b21', 'S5Q05_b22', 'S4Q011_AL']\n",
    "X, y = app_df[invbool_cols].values, app_df['S1Q061nor_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVBOOL\", \"ap\", 'S1Q061nor', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.37037037037037035\n",
      "Max:  0.6111111111111112\n",
      "Avg:  0.45740740740740743\n",
      "Std:  0.06798255511997288 \n",
      "\n",
      "Precision\n",
      "Min:  0.24603174603174602\n",
      "Max:  0.44920634920634916\n",
      "Avg:  0.3460667531615074\n",
      "Std:  0.053752245627564935 \n",
      "\n",
      "Recall\n",
      "Min:  0.2370766488413547\n",
      "Max:  0.49689366786140976\n",
      "Avg:  0.34357960355609074\n",
      "Std:  0.05675554316226803 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.3004884004884005\n",
      "Max:  0.562111801242236\n",
      "Avg:  0.3941043000327235\n",
      "Std:  0.07138396899723186 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVCONT features\n",
    "invcont_cols = ['S2Q01_a_201801', 'S1Q05', 'S5Q05_a01', 'S5Q05_a02', 'S5Q05_a03', 'S5Q05_a04', 'S5Q05_a05', 'S5Q05_a06', 'S5Q05_a24',\n",
    "                'S5Q05_a07', 'S5Q05_a08', 'S5Q05_a09', 'S5Q05_a10', 'S5Q05_a11', 'S5Q05_a12', 'S5Q05_b13', 'S5Q05_b14', 'S5Q05_b15',\n",
    "                'S5Q05_b16', 'S5Q05_b17', 'S5Q05_b18', 'S5Q05_b19', 'S5Q05_b20', 'S5Q05_b21', 'S5Q05_b22', 'S4Q021', 'S4Q022', 'S4Q023',\n",
    "                'S4Q024', 'S4Q025', 'S4Q026']\n",
    "X, y = app_df[invcont_cols].values, app_df['S1Q061nor_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVCONT\", \"ap\", 'S1Q061nor', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1Q06P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8148148148148148\n",
      "Max:  0.9629629629629629\n",
      "Avg:  0.9049382716049386\n",
      "Std:  0.03303478558088803 \n",
      "\n",
      "Precision\n",
      "Min:  0.3137254901960784\n",
      "Max:  0.9629629629629629\n",
      "Avg:  0.4754514114897155\n",
      "Std:  0.1601259113615769 \n",
      "\n",
      "Recall\n",
      "Min:  0.2948717948717949\n",
      "Max:  0.9622641509433962\n",
      "Avg:  0.535885553985073\n",
      "Std:  0.17434898922686803 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.5964653902798233\n",
      "Max:  0.9811320754716981\n",
      "Avg:  0.8301703400800675\n",
      "Std:  0.13916158322353098 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All features\n",
    "# Since the data is continuous we have decided to bin the data into 3 separate categories\n",
    "app_df['S1Q06_P1nor_cat'] = pd.cut(app_df['S1Q06_P1nor'], bins=3, labels=False)\n",
    "\n",
    "all_cols = app_df.columns.drop(['USER', 'S1Q061nor_cat', 'S1Q06_P1nor', 'S1Q06_P1nor_cat'])\n",
    "X, y = app_df[all_cols].values, app_df['S1Q06_P1nor_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"ALL\", \"ap\", 'S1Q06_P1nor_cat', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8148148148148148\n",
      "Max:  0.9444444444444444\n",
      "Avg:  0.8981481481481481\n",
      "Std:  0.03198579360007807 \n",
      "\n",
      "Precision\n",
      "Min:  0.30128205128205127\n",
      "Max:  0.8888888888888888\n",
      "Avg:  0.4249694848378802\n",
      "Std:  0.12321340482916383 \n",
      "\n",
      "Recall\n",
      "Min:  0.30128205128205127\n",
      "Max:  0.9444444444444444\n",
      "Avg:  0.4484055025869941\n",
      "Std:  0.1541889701978858 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.6803921568627451\n",
      "Max:  0.9714285714285714\n",
      "Avg:  0.9181093432179744\n",
      "Std:  0.07720867976878681 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVBOOL features\n",
    "X, y = app_df[invbool_cols].values, app_df['S1Q06_P1nor_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVBOOL\", \"ap\", 'S1Q06_P1nor_cat', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Min:  0.8333333333333334\n",
      "Max:  0.9444444444444444\n",
      "Avg:  0.9\n",
      "Std:  0.030466574516674234 \n",
      "\n",
      "Precision\n",
      "Min:  0.30128205128205127\n",
      "Max:  0.7307692307692308\n",
      "Avg:  0.4093960618009775\n",
      "Std:  0.11271972835734918 \n",
      "\n",
      "Recall\n",
      "Min:  0.29411764705882354\n",
      "Max:  0.9444444444444444\n",
      "Avg:  0.4768958686092295\n",
      "Std:  0.2238408743689157 \n",
      "\n",
      "F1 Score\n",
      "Min:  0.6128571428571429\n",
      "Max:  0.9714285714285714\n",
      "Avg:  0.9086174342711087\n",
      "Std:  0.0895590547715344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INVCONT features\n",
    "X, y = app_df[invcont_cols].values, app_df['S1Q06_P1nor_cat'].values\n",
    "\n",
    "accuracy, precision, recall, f1_score = generate_decision_trees(X, y)\n",
    "    \n",
    "print_results(accuracy, precision, recall, f1_score)\n",
    "\n",
    "new_df = create_df(\"INVCONT\", \"ap\", 'S1Q06_P1nor_cat', accuracy, precision, recall, f1_score)\n",
    "\n",
    "dfs.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor_set</th>\n",
       "      <th>dataset</th>\n",
       "      <th>target</th>\n",
       "      <th>metric</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>avg</th>\n",
       "      <th>stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL</td>\n",
       "      <td>b</td>\n",
       "      <td>S1Q061</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.008189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALL</td>\n",
       "      <td>b</td>\n",
       "      <td>S1Q061</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>0.004415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL</td>\n",
       "      <td>b</td>\n",
       "      <td>S1Q061</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989069</td>\n",
       "      <td>0.018622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALL</td>\n",
       "      <td>b</td>\n",
       "      <td>S1Q061</td>\n",
       "      <td>F1-Score</td>\n",
       "      <td>0.964047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992815</td>\n",
       "      <td>0.012190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INVBOOL</td>\n",
       "      <td>b</td>\n",
       "      <td>S1Q061</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.462346</td>\n",
       "      <td>0.069370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INVBOOL</td>\n",
       "      <td>ap</td>\n",
       "      <td>S1Q06_P1nor_cat</td>\n",
       "      <td>F1-Score</td>\n",
       "      <td>0.680392</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918109</td>\n",
       "      <td>0.077209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INVCONT</td>\n",
       "      <td>ap</td>\n",
       "      <td>S1Q06_P1nor_cat</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.030467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INVCONT</td>\n",
       "      <td>ap</td>\n",
       "      <td>S1Q06_P1nor_cat</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.301282</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.409396</td>\n",
       "      <td>0.112720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INVCONT</td>\n",
       "      <td>ap</td>\n",
       "      <td>S1Q06_P1nor_cat</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.476896</td>\n",
       "      <td>0.223841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INVCONT</td>\n",
       "      <td>ap</td>\n",
       "      <td>S1Q06_P1nor_cat</td>\n",
       "      <td>F1-Score</td>\n",
       "      <td>0.612857</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.908617</td>\n",
       "      <td>0.089559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   predictor_set dataset           target     metric       min       max  \\\n",
       "0            ALL       b           S1Q061   Accuracy  0.981481  1.000000   \n",
       "1            ALL       b           S1Q061  Precision  0.988889  1.000000   \n",
       "2            ALL       b           S1Q061     Recall  0.944444  1.000000   \n",
       "3            ALL       b           S1Q061   F1-Score  0.964047  1.000000   \n",
       "0        INVBOOL       b           S1Q061   Accuracy  0.314815  0.611111   \n",
       "..           ...     ...              ...        ...       ...       ...   \n",
       "3        INVBOOL      ap  S1Q06_P1nor_cat   F1-Score  0.680392  0.971429   \n",
       "0        INVCONT      ap  S1Q06_P1nor_cat   Accuracy  0.833333  0.944444   \n",
       "1        INVCONT      ap  S1Q06_P1nor_cat  Precision  0.301282  0.730769   \n",
       "2        INVCONT      ap  S1Q06_P1nor_cat     Recall  0.294118  0.944444   \n",
       "3        INVCONT      ap  S1Q06_P1nor_cat   F1-Score  0.612857  0.971429   \n",
       "\n",
       "         avg    stddev  \n",
       "0   0.995062  0.008189  \n",
       "1   0.997349  0.004415  \n",
       "2   0.989069  0.018622  \n",
       "3   0.992815  0.012190  \n",
       "0   0.462346  0.069370  \n",
       "..       ...       ...  \n",
       "3   0.918109  0.077209  \n",
       "0   0.900000  0.030467  \n",
       "1   0.409396  0.112720  \n",
       "2   0.476896  0.223841  \n",
       "3   0.908617  0.089559  \n",
       "\n",
       "[92 rows x 8 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df = pd.concat(dfs)\n",
    "concat_df.head(92)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
