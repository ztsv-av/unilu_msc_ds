{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_100 = False\n",
    "num_classes = 100 if load_100 else 10\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if load_100:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_indices = list(range(len(trainset)))\n",
    "train_labels = trainset.targets\n",
    "train_idx, val_idx = train_test_split(train_indices, test_size=0.2, stratify=train_labels, random_state=0)\n",
    "\n",
    "train_subset = Subset(trainset, train_idx)\n",
    "val_subset = Subset(trainset, val_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "validationloader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSGDW\u001b[39;00m(\u001b[43moptim\u001b[49m\u001b[38;5;241m.\u001b[39mSGD):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dampening\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(SGDW, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39mmomentum, dampening\u001b[38;5;241m=\u001b[39mdampening, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39mnesterov)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "class SGDW(optim.SGD):\n",
    "    def __init__(self, params, lr=0.01, momentum=0, dampening=0, weight_decay=0.01, nesterov=False):\n",
    "        super(SGDW, self).__init__(params, lr=lr, momentum=momentum, dampening=dampening, weight_decay=0, nesterov=nesterov)\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        # apply weight decay to parameters before gradient step\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                param.data = param.data.add(-self.weight_decay * group['lr'], param.data)\n",
    "        # apply step\n",
    "        super(SGDW, self).step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        # input_shape: (batch_size,num_channels,image_width,image_height)\n",
    "        #              (batch_size,3,32,32)\n",
    "        # conv block 1\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1) # output shape: (batch_size,64,32,32), weights: (64,3,3,3), bias: (64)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1) # output shape: (batch_size,64,32,32), weights: (64,64,3,3), bias: (64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # output shape: (batch_size,64,16,16)\n",
    "        # conv block 2\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)  # output shape: (batch_size,128,16,16), weights: (128,64,3,3), bias: (128)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1) # output shape: (batch_size,128,16,16), weights: (128,128,3,3), bias: (128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # output shape: (batch_size,128,8,8)\n",
    "        # conv block 3\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) # output shape: (batch_size, 256, 8, 8), weights: (256,128,3,3), bias: (256)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) # output shape: (batch_size, 256, 8, 8), weights: (256,256,3,3), bias: (256)\n",
    "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) # output shape: (batch_size, 256, 8, 8), weights: (256,256,3,3), bias: (256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # output shape: (batch_size,256,4,4)\n",
    "        # conv block 4\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1) # output shape: (batch_size, 512, 4, 4), weights: (512,256,3,3), bias: (512)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1) # output shape: (batch_size, 512, 4, 4), weights: (512,512,3,3), bias: (512)\n",
    "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1) # output shape: (batch_size, 512, 4, 4), weights: (512,512,3,3), bias: (512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # output shape: (batch_size,512,2,2)\n",
    "        # fc 1\n",
    "        self.fc1 = nn.Linear(in_features=512*2*2, out_features=512) # output shape: (batch_size, 512), weights: (2048,512), bias: (512)\n",
    "        # fc 2\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=512) # output shape: (batch_size, 512), weights: (512,512), bias: (512)\n",
    "        # fc 3\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=num_classes) # output shape: (batch_size, 10), weights: (512,10), bias: (10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convs\n",
    "        x = self.pool1(F.relu(self.conv1_2(F.relu(self.conv1_1(x)))))\n",
    "        x = self.pool2(F.relu(self.conv2_2(F.relu(self.conv2_1(x)))))\n",
    "        x = self.pool3(F.relu(self.conv3_3(F.relu(self.conv3_2(F.relu(self.conv3_1(x)))))))\n",
    "        x = self.pool4(F.relu(self.conv4_3(F.relu(self.conv4_2(F.relu(self.conv4_1(x)))))))\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fcs\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.backbone = models.resnet18(weights=None)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainTrainer:\n",
    "    def __init__(self, device, trainloader, validationloader, testloader,\n",
    "                 model, loss, optimizer_type, num_epochs,\n",
    "                 momentum=0.9, weight_decay=0.01, rho=0.9, epsilon=1e-8,\n",
    "                 lr_scheduler_factor=0.7, lr_scheduler_patience=3, early_stop_patience=5, early_stop_min_delta=0.001):\n",
    "        self.device = device\n",
    "        # data\n",
    "        self.trainloader = trainloader\n",
    "        self.validationloader = validationloader\n",
    "        self.testloader = testloader\n",
    "        # model\n",
    "        self.model = model.to(self.device)\n",
    "        # training\n",
    "        self.num_epochs = num_epochs\n",
    "        # loss, optimizer, learning rate scheduler\n",
    "        self.criterion = loss\n",
    "        self.momentum = momentum # momentum value for SGD, default 0.9\n",
    "        self.weight_decay = weight_decay # weight decay in W optimizers\n",
    "        self.rho = rho # decay rate for RMSProp, default 0.9\n",
    "        self.epsilon = epsilon  # small constant for numerical stability, default 1e-8\n",
    "        if optimizer_type == \"adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        elif optimizer_type == \"adamw\":\n",
    "            self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=self.weight_decay)\n",
    "        elif optimizer_type == \"sgd\":\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.01, momentum=self.momentum)\n",
    "        elif optimizer_type == \"sgdw\":\n",
    "            self.optimizer = SGDW(self.model.parameters(), lr=0.01, momentum=self.momentum, weight_decay=self.weight_decay)\n",
    "        elif optimizer_type == \"adagrad\":\n",
    "            self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.01, eps=self.epsilon)\n",
    "        elif optimizer_type == \"rmsprop\":\n",
    "            self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001, alpha=self.rho, eps=self.epsilon)\n",
    "        self.lr_scheduler_factor = lr_scheduler_factor\n",
    "        self.lr_scheduler_patience = lr_scheduler_patience\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=self.lr_scheduler_factor, patience=self.lr_scheduler_patience)\n",
    "        # early stopping variables\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop_count = 0\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_min_delta = early_stop_min_delta\n",
    "        # metrics\n",
    "        self.training_losses = []\n",
    "        self.training_accuracies = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracies = []\n",
    "        self.epoch_times = []\n",
    "        self.test_accuracy = 0\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # training\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            self.model.train()\n",
    "            for images, labels in self.trainloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                # forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                # backward pass and optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # loss and accuracy\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            # training metrics\n",
    "            epoch_loss = running_loss / len(self.trainloader)\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "            epoch_duration = time.time() - start_time\n",
    "            self.training_losses.append(epoch_loss)\n",
    "            self.training_accuracies.append(epoch_accuracy)\n",
    "            self.epoch_times.append(epoch_duration)\n",
    "\n",
    "            # validation\n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_images, val_labels in self.validationloader:\n",
    "                    val_images, val_labels = val_images.to(self.device), val_labels.to(self.device)\n",
    "                    # forward pass\n",
    "                    val_outputs = self.model(val_images)\n",
    "                    val_loss = self.criterion(val_outputs, val_labels)\n",
    "                    # validation loss and accuracy\n",
    "                    val_running_loss += val_loss.item()\n",
    "                    _, val_predicted = val_outputs.max(1)\n",
    "                    val_total += val_labels.size(0)\n",
    "                    val_correct += val_predicted.eq(val_labels).sum().item()\n",
    "            # validation metrics\n",
    "            val_epoch_loss = val_running_loss / len(self.validationloader)\n",
    "            val_epoch_accuracy = 100 * val_correct / val_total\n",
    "            self.validation_losses.append(val_epoch_loss)\n",
    "            self.validation_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "            # plot metrics\n",
    "            self.plot_metrics()\n",
    "            # scheduler step based on validation loss\n",
    "            self.scheduler.step(val_epoch_loss)\n",
    "            # early stopping\n",
    "            if val_epoch_loss < self.best_loss - self.early_stop_min_delta:\n",
    "                self.best_loss = val_epoch_loss\n",
    "                self.early_stop_count = 0\n",
    "            else:\n",
    "                self.early_stop_count += 1\n",
    "                print(f\"Early stopping patience count: {self.early_stop_count}/{self.early_stop_patience}\")\n",
    "                if self.early_stop_count >= self.early_stop_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    def test(self):\n",
    "        print(\"Starting testing...\")\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.testloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        self.test_accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy of the model: {self.test_accuracy:.2f}%\")\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(16, 10))\n",
    "\n",
    "        # training loss\n",
    "        plt.subplot(3, 2, 1)\n",
    "        plt.plot(range(1, len(self.training_losses) + 1), self.training_losses, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "        plt.title(\"Training Loss Convergence\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        # validation loss\n",
    "        plt.subplot(3, 2, 2)\n",
    "        plt.plot(range(1, len(self.validation_losses) + 1), self.validation_losses, marker=\"o\", linestyle=\"-\", color=\"orange\")\n",
    "        plt.title(\"Validation Loss Convergence\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        # training accuracy\n",
    "        plt.subplot(3, 2, 3)\n",
    "        plt.plot(range(1, len(self.training_accuracies) + 1), self.training_accuracies, marker=\"o\", linestyle=\"-\", color=\"g\")\n",
    "        plt.title(\"Training Accuracy per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.grid(True)\n",
    "        # validation accuracy\n",
    "        plt.subplot(3, 2, 4)\n",
    "        plt.plot(range(1, len(self.validation_accuracies) + 1), self.validation_accuracies, marker=\"o\", linestyle=\"-\", color=\"purple\")\n",
    "        plt.title(\"Validation Accuracy per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.grid(True)\n",
    "        # total runtime\n",
    "        plt.subplot(3, 2, (5, 6))\n",
    "        total_times = [sum(self.epoch_times[:i + 1]) for i in range(len(self.epoch_times))]\n",
    "        plt.plot(range(1, len(total_times) + 1), total_times, marker=\"o\", linestyle=\"-\", color=\"r\")\n",
    "        plt.title(\"Total Runtime per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Total Time (seconds)\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\"adam\", \"sgd\", \"adagrad\", \"rmsprop\"]\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "epoch_times = []\n",
    "for optimizer in optimizers:\n",
    "    success = False\n",
    "    tries = 0\n",
    "    while not success:\n",
    "      model = MainNetwork(num_classes=num_classes)\n",
    "      loss = nn.CrossEntropyLoss()\n",
    "      trainer = MainTrainer(device, trainloader, validationloader, testloader, model, loss, optimizer, num_epochs)\n",
    "      trainer.train()\n",
    "      tries += 1\n",
    "      if len(trainer.training_losses) >= 10:\n",
    "        success = True\n",
    "        print(f\"{optimizer} success\")\n",
    "      else:\n",
    "        print(f\"{optimizer} fail\")\n",
    "      if tries > 5:\n",
    "        print(\"Exceeded number of tries.\")\n",
    "    validation_losses.append(trainer.validation_losses)\n",
    "    training_accuracies.append(trainer.training_accuracies)\n",
    "    validation_accuracies.append(trainer.validation_accuracies)\n",
    "    epoch_times.append(trainer.epoch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainTrainer:\n",
    "    def __init__(self, device, trainloader, validationloader, testloader,\n",
    "                 model, loss, optimizer_type, num_epochs,\n",
    "                 momentum=0.9, weight_decay=0.01, rho=0.9, epsilon=1e-8,\n",
    "                 lr_scheduler_factor=0.7, lr_scheduler_patience=3, early_stop_patience=6, early_stop_min_delta=0.001):\n",
    "        self.device = device\n",
    "        # data\n",
    "        self.trainloader = trainloader\n",
    "        self.validationloader = validationloader\n",
    "        self.testloader = testloader\n",
    "        # model\n",
    "        self.model = model.to(self.device)\n",
    "        # training\n",
    "        self.num_epochs = num_epochs\n",
    "        # loss, optimizer, learning rate scheduler\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.criterion = loss\n",
    "        self.momentum = momentum # momentum value for SGD, default 0.9\n",
    "        self.weight_decay = weight_decay # weight decay in W optimizers\n",
    "        self.rho = rho # decay rate for RMSProp, default 0.9\n",
    "        self.epsilon = epsilon  # small constant for numerical stability, default 1e-8\n",
    "        if optimizer_type == \"adam\":\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        elif optimizer_type == \"adamw\":\n",
    "            self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=self.weight_decay)\n",
    "        elif optimizer_type == \"sgd\":\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.01, momentum=self.momentum)\n",
    "        elif optimizer_type == \"sgdw\":\n",
    "            self.optimizer = SGDW(self.model.parameters(), lr=0.01, momentum=self.momentum, weight_decay=self.weight_decay)\n",
    "        elif optimizer_type == \"adagrad\":\n",
    "            self.optimizer = optim.Adagrad(self.model.parameters(), lr=0.01, eps=self.epsilon)\n",
    "        elif optimizer_type == \"rmsprop\":\n",
    "            self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001, alpha=self.rho, eps=self.epsilon)\n",
    "        self.lr_scheduler_factor = lr_scheduler_factor\n",
    "        self.lr_scheduler_patience = lr_scheduler_patience\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=self.lr_scheduler_factor, patience=self.lr_scheduler_patience)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, \n",
    "        #     T_0=10,       # number of epochs for the first restart\n",
    "        #     T_mult=2,     # factor by which the number of epochs increases after each restart\n",
    "        #     eta_min=1e-6  # minimum learning rate\n",
    "        # )\n",
    "        # early stopping variables\n",
    "        self.check_early_stopping = False\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop_count = 0\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_min_delta = early_stop_min_delta\n",
    "        # metrics\n",
    "        self.training_losses = []\n",
    "        self.training_accuracies = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracies = []\n",
    "        self.epoch_times = []\n",
    "        self.test_accuracy = 0\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # training\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            self.model.train()\n",
    "            for images, labels in self.trainloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                # forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                # backward pass and optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # loss and accuracy\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            # training metrics\n",
    "            epoch_loss = running_loss / len(self.trainloader)\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "            epoch_duration = time.time() - start_time\n",
    "            self.training_losses.append(epoch_loss)\n",
    "            self.training_accuracies.append(epoch_accuracy)\n",
    "            self.epoch_times.append(epoch_duration)\n",
    "\n",
    "            # validation\n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_images, val_labels in self.validationloader:\n",
    "                    val_images, val_labels = val_images.to(self.device), val_labels.to(self.device)\n",
    "                    # forward pass\n",
    "                    val_outputs = self.model(val_images)\n",
    "                    val_loss = self.criterion(val_outputs, val_labels)\n",
    "                    # validation loss and accuracy\n",
    "                    val_running_loss += val_loss.item()\n",
    "                    _, val_predicted = val_outputs.max(1)\n",
    "                    val_total += val_labels.size(0)\n",
    "                    val_correct += val_predicted.eq(val_labels).sum().item()\n",
    "            # validation metrics\n",
    "            val_epoch_loss = val_running_loss / len(self.validationloader)\n",
    "            val_epoch_accuracy = 100 * val_correct / val_total\n",
    "            self.validation_losses.append(val_epoch_loss)\n",
    "            self.validation_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "            # plot metrics\n",
    "            self.plot_metrics()\n",
    "            # scheduler step based on validation loss\n",
    "            self.scheduler.step(val_epoch_loss)\n",
    "            # early stopping\n",
    "            if self.check_early_stopping:\n",
    "              if val_epoch_loss < self.best_loss - self.early_stop_min_delta:\n",
    "                  self.best_loss = val_epoch_loss\n",
    "                  self.early_stop_count = 0\n",
    "              else:\n",
    "                  self.early_stop_count += 1\n",
    "                  print(f\"Early stopping patience count: {self.early_stop_count}/{self.early_stop_patience}\")\n",
    "                  if self.early_stop_count >= self.early_stop_patience:\n",
    "                      print(\"Early stopping triggered.\")\n",
    "                      break\n",
    "\n",
    "    def test(self):\n",
    "        print(\"Starting testing...\")\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.testloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        self.test_accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy of the model: {self.test_accuracy:.2f}%\")\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(16, 10))\n",
    "\n",
    "        # training loss\n",
    "        plt.subplot(3, 2, 1)\n",
    "        plt.plot(range(1, len(self.training_losses) + 1), self.training_losses, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "        plt.title(\"Training Loss Convergence\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        # validation loss\n",
    "        plt.subplot(3, 2, 2)\n",
    "        plt.plot(range(1, len(self.validation_losses) + 1), self.validation_losses, marker=\"o\", linestyle=\"-\", color=\"orange\")\n",
    "        plt.title(\"Validation Loss Convergence\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        # training accuracy\n",
    "        plt.subplot(3, 2, 3)\n",
    "        plt.plot(range(1, len(self.training_accuracies) + 1), self.training_accuracies, marker=\"o\", linestyle=\"-\", color=\"g\")\n",
    "        plt.title(\"Training Accuracy per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.grid(True)\n",
    "        # validation accuracy\n",
    "        plt.subplot(3, 2, 4)\n",
    "        plt.plot(range(1, len(self.validation_accuracies) + 1), self.validation_accuracies, marker=\"o\", linestyle=\"-\", color=\"purple\")\n",
    "        plt.title(\"Validation Accuracy per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.grid(True)\n",
    "        # total runtime\n",
    "        plt.subplot(3, 2, (5, 6))\n",
    "        total_times = [sum(self.epoch_times[:i + 1]) for i in range(len(self.epoch_times))]\n",
    "        plt.plot(range(1, len(total_times) + 1), total_times, marker=\"o\", linestyle=\"-\", color=\"r\")\n",
    "        plt.title(\"Total Runtime per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Total Time (seconds)\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.suptitle(self.optimizer_type)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\"adam\", \"adamw\", \"sgd\", \"sgdw\", \"rmsprop\"]\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "epoch_times = []\n",
    "for optimizer_type in optimizers:\n",
    "    model = ResNet(num_classes=num_classes)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    trainer = MainTrainer(device, trainloader, validationloader, testloader, model, loss, optimizer_type, num_epochs)\n",
    "    trainer.train()\n",
    "    training_losses.append(trainer.training_losses)\n",
    "    validation_losses.append(trainer.validation_losses)\n",
    "    training_accuracies.append(trainer.training_accuracies)\n",
    "    validation_accuracies.append(trainer.validation_accuracies)\n",
    "    epoch_times.append(trainer.epoch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"blue\", \"green\", \"red\", \"purple\", \"orange\"]\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16, 10))\n",
    "\n",
    "# training and validation losses\n",
    "for i, optimizer_name in enumerate(optimizers):\n",
    "    axs[0, 0].plot(training_losses[i], color=colors[i], label=optimizer_name)\n",
    "    axs[0, 1].plot(validation_losses[i], color=colors[i], label=optimizer_name)\n",
    "axs[0, 0].set_title(\"Training Loss\")\n",
    "axs[0, 0].set_xlabel(\"Epoch\")\n",
    "axs[0, 0].set_ylabel(\"Loss\")\n",
    "axs[0, 0].legend()\n",
    "axs[0, 1].set_title(\"Validation Loss\")\n",
    "axs[0, 1].set_xlabel(\"Epoch\")\n",
    "axs[0, 1].set_ylabel(\"Loss\")\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# training and validation accuracies\n",
    "for i, optimizer_name in enumerate(optimizers):\n",
    "    axs[1, 0].plot(training_accuracies[i], color=colors[i], label=optimizer_name)\n",
    "    axs[1, 1].plot(validation_accuracies[i], color=colors[i], label=optimizer_name)\n",
    "axs[1, 0].set_title(\"Training Accuracy\")\n",
    "axs[1, 0].set_xlabel(\"Epoch\")\n",
    "axs[1, 0].set_ylabel(\"Accuracy\")\n",
    "axs[1, 0].legend()\n",
    "axs[1, 1].set_title(\"Validation Accuracy\")\n",
    "axs[1, 1].set_xlabel(\"Epoch\")\n",
    "axs[1, 1].set_ylabel(\"Accuracy\")\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# epoch times\n",
    "for i, optimizer_name in enumerate(optimizers):\n",
    "    axs[2, 0].plot(epoch_times[i], color=colors[i], label=optimizer_name)\n",
    "axs[2, 0].set_title(\"Epoch Times\")\n",
    "axs[2, 0].set_xlabel(\"Epoch\")\n",
    "axs[2, 0].set_ylabel(\"Time (s)\")\n",
    "axs[2, 0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaLayerConv3d\n",
    "\n",
    "Number of trainable parameters:\n",
    "* **Conv3d**:\n",
    "    * **Weights**:  $\\text{in\\_channels} \\times \\text{out\\_channels} \\times \\text{kernel size} = \\text{num\\_optimizers} \\times \\text{num\\_optimizers} = \\text{num\\_optimizers}^2$\n",
    "    * **Biases**: $\\text{out\\_channels} = 1$\n",
    "    * **Total**: $\\text{num\\_optimizers}^2 + 1$\n",
    "* **FC**:\n",
    "    * **1st FC**:  $(\\text{input\\_features}\\times \\text{output\\_features})+\\text{output\\_features}=\\text{num\\_optimizers}\\times \\text{out\\_channels}\\times \\text{bias\\_neurons}+\\text{bias\\_neurons}$\n",
    "    * **2nd FC**: $(\\text{input\\_size}\\times \\text{output\\_size})+\\text{output\\_size}=\\text{bias\\_neurons}\\times \\text{out\\_channels}+\\text{out\\_channels}$\n",
    "    * **Total**: $\\left(\\text{num\\_optimizers}\\times \\text{out\\_channels}\\times \\text{bias\\_neurons} + \\text{bias\\_neurons} \\right)+\\left(\\text{bias\\_neurons}\\times \\text{out\\_channels}+\\text{out\\_channels}\\right)$\n",
    "\n",
    "**Total**: \n",
    "$$\\left[\\text{num\\_optimizers}^2 + 1\\right] + \\left[\\left(\\text{num\\_optimizers}\\times \\text{out\\_channels}\\times \\text{bias\\_neurons} + \\text{bias\\_neurons}\\right)+\\left(\\text{bias\\_neurons}\\times \\text{out\\_channels}+\\text{out\\_channels}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLayerConv3d(nn.Module):\n",
    "    def __init__(self, num_optimizers, output_channels, input_channels, kernel_size, bias_neurons=64):\n",
    "        super(MetaLayerConv3d, self).__init__()\n",
    "    \n",
    "        self.conv3d = nn.Conv3d(\n",
    "            in_channels=num_optimizers, \n",
    "            out_channels=1, \n",
    "            kernel_size=(num_optimizers, 1, 1), \n",
    "            stride=(1, 1, 1),\n",
    "            padding=(1, 0, 0)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_optimizers * output_channels, bias_neurons)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(bias_neurons, output_channels)\n",
    "        \n",
    "        self.output_channels = output_channels\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def forward(self, weight_gradients, bias_gradients):\n",
    "        # --- Weights Gradients ---\n",
    "        # expect weight_gradients input of shape (num_optimizers, output_channels, input_channels, kernel_size, kernel_size)\n",
    "        # add a batch dimension: shape becomes (1, num_optimizers, output_channels, input_channels, kernel_size, kernel_size)\n",
    "        weight_gradients = weight_gradients.unsqueeze(0)\n",
    "        # flatten last two dimensions to get shape (1, num_optimizers, output_channels, input_channels, kernel_size * kernel_size)\n",
    "        weight_gradients = weight_gradients.view(1, weight_gradients.size(1), weight_gradients.size(2), weight_gradients.size(3), -1)\n",
    "        # pass through Conv3d to combine optimizer channels, shape becomes (1, 1, output_channels, input_channels, kernel_size * kernel_size)\n",
    "        conv_output = self.conv3d(weight_gradients)\n",
    "        # remove batch dimension and channel\n",
    "        weight_output = conv_output.squeeze(0).squeeze(0)\n",
    "        # unflatten to (output_channels, input_channels, kernel_size, kernel_size)\n",
    "        weight_output = weight_output.view(self.output_channels, self.input_channels, self.kernel_size, self.kernel_size)\n",
    "\n",
    "        # --- Bias Gradients ---\n",
    "        # expect bias_gradients input of shape (num_optimizers, output_channels)\n",
    "        # flatten to shape (num_optimizers * output_channels)\n",
    "        bias_gradients = bias_gradients.view(-1)\n",
    "        # pass through the fully connected layers\n",
    "        fc_output = self.fc1(bias_gradients)\n",
    "        fc_output = self.relu(fc_output)\n",
    "        bias_output = self.fc2(fc_output)\n",
    "    \n",
    "        return weight_output, bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaLayerConv2d\n",
    "\n",
    "Number of trainable parameters:\n",
    "* **Conv2d**:\n",
    "    * **Weights**:  $\\text{in\\_features} \\times \\text{out\\_features} \\times \\text{kernel size} = \\text{num\\_optimizers}$\n",
    "    * **Biases**: $\\text{out\\_features} = 1$\n",
    "    * **Total**: $\\text{num\\_optimizers} + 1$\n",
    "* **FC**:\n",
    "    * **1st FC**:  $\\text{num\\_optimizers}\\times \\text{out\\_features} \\times \\text{bias\\_neurons}+\\text{bias\\_neurons}$\n",
    "    * **2nd FC**: $\\text{bias\\_neurons}\\times \\text{out\\_features}+\\text{out\\_features}$\n",
    "    * **Total**: $\\left(\\text{num\\_optimizers}\\times \\text{out\\_features} \\times \\text{bias\\_neurons}+\\text{bias\\_neurons}\\right)+\\left(\\text{bias\\_neurons}\\times \\text{out\\_features}+\\text{out\\_features}\\right)$\n",
    "\n",
    "**Total**: \n",
    "$$\\left[\\text{num\\_optimizers} + 1\\right] + \\left[\\left(\\text{num\\_optimizers}\\times \\text{out\\_features} \\times \\text{bias\\_neurons}+\\text{bias\\_neurons}\\right)+\\left(\\text{bias\\_neurons}\\times \\text{out\\_features}+\\text{out\\_features}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLayerConv2d(nn.Module):\n",
    "    def __init__(self, num_optimizers, out_features, bias_neurons=64):\n",
    "        super(MetaLayerConv2d, self).__init__()\n",
    "    \n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=num_optimizers, \n",
    "            out_channels=1, \n",
    "            kernel_size=(1, 1), \n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_optimizers * out_features, bias_neurons)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(bias_neurons, out_features)\n",
    "        \n",
    "    def forward(self, weight_gradients, bias_gradients):\n",
    "        # --- Weights Gradients ---\n",
    "        # expect weight_gradients input of shape (num_optimizers, in_features, out_features)\n",
    "        # add a batch dimension: shape becomes (1, num_optimizers, in_features, out_features)\n",
    "        weight_gradients = weight_gradients.unsqueeze(0)\n",
    "        # pass through Conv2d to combine optimizer channels, shape becomes (1, 1, in_features, out_features)\n",
    "        conv_output = self.conv2d(weight_gradients)\n",
    "        # remove batch dimension and channel\n",
    "        weight_output = conv_output.squeeze(0).squeeze(0)\n",
    "\n",
    "        # --- Bias Gradients ---\n",
    "        # expect bias_gradients input of shape (num_optimizers, in_features)\n",
    "        # flatten to shape (num_optimizers * in_features)\n",
    "        bias_gradients = bias_gradients.view(-1)\n",
    "        # pass through the fully connected layers\n",
    "        fc_output = self.fc1(bias_gradients)\n",
    "        fc_output = self.relu(fc_output)\n",
    "        bias_output = self.fc2(fc_output)\n",
    "    \n",
    "        return weight_output, bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Meta Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_meta_layer_nns(main_model, num_optimizers):\n",
    "    meta_layer_nns = []\n",
    "    for layer in main_model.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            meta_layer = MetaLayerConv3d(\n",
    "                num_optimizers=num_optimizers,\n",
    "                output_channels=layer.out_channels,\n",
    "                input_channels=layer.in_channels,\n",
    "                kernel_size=layer.kernel_size[0],\n",
    "                bias_neurons=64\n",
    "            )\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            meta_layer = MetaLayerConv2d(\n",
    "                num_optimizers=num_optimizers,\n",
    "                out_features=layer.out_features,\n",
    "                bias_neurons=64\n",
    "            )\n",
    "        else:\n",
    "            continue\n",
    "        meta_layer_nns.append(meta_layer)\n",
    "    return nn.ModuleList(meta_layer_nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model_parameters, optimizer_type=\"adam\", lr=0.001, momentum=0.9, rho=0.9, epsilon=1e-8):\n",
    "        self.model_parameters = model_parameters\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.multiply_lr = True\n",
    "        self.optimizer, self.optimizer_function = self._initialize_optimizer(optimizer_type)\n",
    "\n",
    "    def _initialize_optimizer(self, optimizer_type):\n",
    "        \"\"\"Initializes optimizer and corresponding gradient function based on optimizer type.\"\"\"\n",
    "        if optimizer_type == \"adam\":\n",
    "            optimizer = optim.Adam(self.model_parameters, lr=self.lr)\n",
    "            optimizer_function = self.adam_gradients\n",
    "            self.multiply_lr = True\n",
    "        elif optimizer_type == \"sgd\":\n",
    "            optimizer = optim.SGD(self.model_parameters, lr=self.lr, momentum=self.momentum)\n",
    "            optimizer_function = self.sgd_momentum_gradients\n",
    "            self.multiply_lr = False\n",
    "        elif optimizer_type == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(self.model_parameters, lr=self.lr, eps=self.epsilon)\n",
    "            optimizer_function = self.adagrad_gradients\n",
    "            self.multiply_lr = True\n",
    "        elif optimizer_type == \"rmsprop\":\n",
    "            optimizer = optim.RMSprop(self.model_parameters, lr=self.lr, alpha=self.rho, eps=self.epsilon)\n",
    "            optimizer_function = self.rmsprop_gradients\n",
    "            self.multiply_lr = True\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer type '{optimizer_type}' is not supported.\")\n",
    "        return optimizer, optimizer_function\n",
    "\n",
    "    def sgd_momentum_gradients(self):\n",
    "        optimizer_gradients = []\n",
    "        for param in self.model_parameters:\n",
    "            if param.grad is not None:\n",
    "                # initialize velocity in optimizer state if not already present\n",
    "                if \"velocity\" not in self.optimizer.state[param]:\n",
    "                    self.optimizer.state[param][\"velocity\"] = torch.zeros_like(param.grad)\n",
    "                # retrieve the current velocity and momentum factor\n",
    "                velocity = self.optimizer.state[param][\"velocity\"]\n",
    "                lr = self.optimizer.defaults[\"lr\"]\n",
    "                # update the velocity\n",
    "                velocity.mul_(self.momentum).add_(param.grad, alpha=lr)  # v_t = gamma * v_{t-1} + eta * g_t\n",
    "                # append the updated velocity (momentum-adjusted gradient)\n",
    "                optimizer_gradients.append(velocity.clone())  # clone to avoid in-place modifications\n",
    "        return optimizer_gradients\n",
    "\n",
    "    def adagrad_gradients(self):\n",
    "        optimizer_gradients = []\n",
    "        for param in self.model_parameters:\n",
    "            if param.grad is not None:\n",
    "                # initialize accumulated sum of squared gradients if not present\n",
    "                if \"sum_sq_grads\" not in self.optimizer.state[param]:\n",
    "                    self.optimizer.state[param][\"sum_sq_grads\"] = torch.zeros_like(param.grad)\n",
    "                # retrieve the accumulated squared gradients\n",
    "                sum_sq_grads = self.optimizer.state[param][\"sum_sq_grads\"]\n",
    "                # accumulate the squared gradients\n",
    "                sum_sq_grads.addcmul_(param.grad, param.grad)  # G_t = G_{t-1} + g_t^2\n",
    "                # compute AdaGrad-adjusted gradient\n",
    "                adagrad_adjusted_grad = param.grad / (sum_sq_grads.sqrt() + self.epsilon)\n",
    "                # append the adjusted gradient (with learning rate scaling)\n",
    "                optimizer_gradients.append(adagrad_adjusted_grad.clone())\n",
    "\n",
    "        return optimizer_gradients\n",
    "\n",
    "    def rmsprop_gradients(self):\n",
    "        optimizer_gradients = []\n",
    "        for param in self.model_parameters:\n",
    "            if param.grad is not None:\n",
    "                # initialize the moving average of squared gradients if not present\n",
    "                if \"square_avg\" not in self.optimizer.state[param]:\n",
    "                    self.optimizer.state[param][\"square_avg\"] = torch.zeros_like(param.grad)\n",
    "                # retrieve the moving average of squared gradients\n",
    "                square_avg = self.optimizer.state[param][\"square_avg\"]\n",
    "                # update the moving average of squared gradients\n",
    "                square_avg.mul_(self.rho).addcmul_(1 - self.rho, param.grad, param.grad)\n",
    "                # compute RMSprop-adjusted gradient\n",
    "                rmsprop_adjusted_grad = param.grad / (square_avg.sqrt() + self.epsilon)\n",
    "                # append the adjusted gradient with learning rate scaling\n",
    "                optimizer_gradients.append(rmsprop_adjusted_grad.clone())\n",
    "        return optimizer_gradients\n",
    "\n",
    "    def adam_gradients(self):\n",
    "        optimizer_gradients = []\n",
    "        for param in self.model_parameters:\n",
    "            if param.grad is not None:\n",
    "                # initialize exp_avg, exp_avg_sq, and step in optimizer state if not already done\n",
    "                if \"exp_avg\" not in self.optimizer.state[param]:\n",
    "                    self.optimizer.state[param][\"exp_avg\"] = torch.zeros_like(param.grad)\n",
    "                if \"exp_avg_sq\" not in self.optimizer.state[param]:\n",
    "                    self.optimizer.state[param][\"exp_avg_sq\"] = torch.zeros_like(param.grad)\n",
    "                if \"step\" not in self.optimizer.state[param]:\n",
    "                    self.optimizer.state[param][\"step\"] = 0\n",
    "                # retrieve optimizer state variables\n",
    "                exp_avg = self.optimizer.state[param][\"exp_avg\"]\n",
    "                exp_avg_sq = self.optimizer.state[param][\"exp_avg_sq\"]\n",
    "                step = self.optimizer.state[param][\"step\"]\n",
    "                beta1, beta2 = self.optimizer.defaults[\"betas\"]\n",
    "                eps = self.optimizer.defaults[\"eps\"]\n",
    "                # increment step count\n",
    "                step += 1\n",
    "                self.optimizer.state[param][\"step\"] = step\n",
    "                # update exp_avg and exp_avg_sq according to Adam rules\n",
    "                exp_avg.mul_(beta1).add_(param.grad, alpha=(1 - beta1))  # update first moment\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(param.grad, param.grad, value=(1 - beta2))  # update second moment\n",
    "                # compute bias-corrected first and second moments\n",
    "                exp_avg_corrected = exp_avg / (1 - beta1 ** step)\n",
    "                exp_avg_sq_corrected = exp_avg_sq / (1 - beta2 ** step)\n",
    "                # calculate the optimizer-adjusted gradient\n",
    "                optimizer_grad = exp_avg_corrected / (exp_avg_sq_corrected.sqrt() + eps)\n",
    "                optimizer_gradients.append(optimizer_grad.clone())  # clone to avoid in-place modifications\n",
    "        return optimizer_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = MainNetwork(num_classes=10)\n",
    "main_optimizers = [\n",
    "    Optimizer(main_model.parameters(), optimizer_type=\"adam\", lr=0.001, epsilon=1e-8).optimizer,\n",
    "    Optimizer(main_model.parameters(), optimizer_type=\"sgd\", lr=0.01, momentum=0.9).optimizer,\n",
    "    Optimizer(main_model.parameters(), optimizer_type=\"adagrad\", lr=0.01, epsilon=1e-8).optimizer,\n",
    "    Optimizer(main_model.parameters(), optimizer_type=\"rmsprop\", lr=0.001, rho=0.9, epsilon=1e-8).optimizer\n",
    "]\n",
    "meta_layer_nns = initialize_meta_layer_nns(main_model, len(main_optimizers))\n",
    "meta_optimizers = [Optimizer(meta_layer_nn.parameters(), optimizer_type=\"adam\", lr=0.001, epsilon=1e-8).optimizer for meta_layer_nn in meta_layer_nns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>Number of Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Main Model</td>\n",
       "      <td>conv1_1.weight</td>\n",
       "      <td>1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Main Model</td>\n",
       "      <td>conv1_1.bias</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Main Model</td>\n",
       "      <td>conv1_2.weight</td>\n",
       "      <td>36864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Main Model</td>\n",
       "      <td>conv1_2.bias</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Main Model</td>\n",
       "      <td>conv2_1.weight</td>\n",
       "      <td>73728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Meta Layer NN 13</td>\n",
       "      <td>fc2.bias</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Main Total</th>\n",
       "      <td>Main Model</td>\n",
       "      <td>-</td>\n",
       "      <td>8952138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Meta Layers Total</th>\n",
       "      <td>Meta Layers</td>\n",
       "      <td>-</td>\n",
       "      <td>719237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall Total</th>\n",
       "      <td>Main Model + Meta Layers</td>\n",
       "      <td>-</td>\n",
       "      <td>9671375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Increase %</th>\n",
       "      <td>Overall Total / Main Total</td>\n",
       "      <td>-</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Model      Layer Name  \\\n",
       "0                                      Main Model  conv1_1.weight   \n",
       "1                                      Main Model    conv1_1.bias   \n",
       "2                                      Main Model  conv1_2.weight   \n",
       "3                                      Main Model    conv1_2.bias   \n",
       "4                                      Main Model  conv2_1.weight   \n",
       "...                                           ...             ...   \n",
       "103                              Meta Layer NN 13        fc2.bias   \n",
       "Main Total                             Main Model               -   \n",
       "All Meta Layers Total                 Meta Layers               -   \n",
       "Overall Total            Main Model + Meta Layers               -   \n",
       "Increase %             Overall Total / Main Total               -   \n",
       "\n",
       "                       Number of Parameters  \n",
       "0                                      1728  \n",
       "1                                        64  \n",
       "2                                     36864  \n",
       "3                                        64  \n",
       "4                                     73728  \n",
       "...                                     ...  \n",
       "103                                      10  \n",
       "Main Total                          8952138  \n",
       "All Meta Layers Total                719237  \n",
       "Overall Total                       9671375  \n",
       "Increase %                                8  \n",
       "\n",
       "[108 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_model_params(model, model_name=\"Model\"):\n",
    "    table_data = []\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param_count = param.numel()\n",
    "            total_params += param_count\n",
    "            table_data.append({\"Model\": model_name, \"Layer Name\": name, \"Number of Parameters\": param_count})\n",
    "    return total_params, table_data\n",
    "\n",
    "main_model_total, full_table_data = count_model_params(main_model, model_name=\"Main Model\")\n",
    "total_meta_nns = 0\n",
    "for i, meta_layer_nn in enumerate(meta_layer_nns):\n",
    "    meta_total, meta_table_data = count_model_params(meta_layer_nn, model_name=f\"Meta Layer NN {i+1}\")\n",
    "    full_table_data.extend(meta_table_data)\n",
    "    total_meta_nns += meta_total\n",
    "\n",
    "df = pd.DataFrame(full_table_data)\n",
    "df.loc[\"Main Total\"] = [\"Main Model\", \"-\", main_model_total]\n",
    "df.loc[\"All Meta Layers Total\"] = [\"Meta Layers\", \"-\", total_meta_nns]\n",
    "df.loc[\"Overall Total\"] = [\"Main Model + Meta Layers\", \"-\", main_model_total + total_meta_nns]\n",
    "df.loc[\"Increase %\"] = [\"Overall Total / Main Total\", \"-\", int(((main_model_total + total_meta_nns) / main_model_total - 1)*100)]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTrainer:\n",
    "    def __init__(self, device, trainloader, validationloader, testloader, \n",
    "                 main_model, meta_layer_nns, \n",
    "                 loss, \n",
    "                 main_optimizers, meta_optimizers, \n",
    "                 num_epochs,\n",
    "                 lr_scheduler_factor=0.5, lr_scheduler_patience=2, \n",
    "                 early_stop_patience=5, early_stop_min_delta=0.001):\n",
    "        self.device = device\n",
    "        # data\n",
    "        self.trainloader = trainloader\n",
    "        self.validationloader = validationloader\n",
    "        self.testloader = testloader\n",
    "        # models\n",
    "        self.main_model = main_model.to(device)\n",
    "        self.meta_layer_nns = meta_layer_nns\n",
    "        for i, meta_layer_nn in enumerate(self.meta_layer_nns):\n",
    "            self.meta_layer_nns[i] = meta_layer_nn.to(device)\n",
    "        # loss, optimizers, meta optimizers, learning rate scheduler\n",
    "        self.criterion = loss\n",
    "        self.main_optimizers = main_optimizers\n",
    "        self.meta_optimizers = meta_optimizers\n",
    "        self.main_schedulers = [ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_scheduler_factor, patience=lr_scheduler_patience) for optimizer in self.main_optimizers]\n",
    "        self.meta_schedulers = [ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_scheduler_factor, patience=lr_scheduler_patience) for optimizer in self.meta_optimizers]\n",
    "        # training\n",
    "        self.num_epochs = num_epochs\n",
    "        # early stopping variables\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop_count = 0\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_min_delta = early_stop_min_delta\n",
    "        # metrics\n",
    "        self.training_losses = []\n",
    "        self.training_accuracies = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracies = []\n",
    "        self.epoch_times = []\n",
    "        self.test_accuracy = 0\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting meta training...\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            # main model training\n",
    "            self.main_model.train()\n",
    "            for meta_layer_nn in self.meta_layer_nns:\n",
    "                meta_layer_nn.train()\n",
    "            for images, labels in self.trainloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                # step 1: forward pass and loss computation for the main model\n",
    "                outputs = self.main_model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                # step 2: compute raw gradients\n",
    "                raw_gradients = torch.autograd.grad(loss, self.main_model.parameters(), create_graph=True)\n",
    "                # step 3: preprocess gradients using each optimizer\n",
    "                gradients_optimizers = []\n",
    "                for optimizer_name, optimizer in self.optimizers.items():\n",
    "                    optimizer_gradients = [grad.clone() for grad in raw_gradients]  # clone to avoid in-place modification\n",
    "                    gradients_optimizers.append(optimizer_gradients)\n",
    "                # step 4: pass each layer's gradients through its meta-layer NN\n",
    "                meta_layer_outputs = []\n",
    "                for i, (meta_layer_nn, weight_grad, bias_grad) in enumerate(zip(self.meta_layer_nns, gradients_optimizers[0], gradients_optimizers[1])):\n",
    "                    # Get outputs for weights and biases\n",
    "                    weight_output, bias_output = meta_layer_nn(weight_grad, bias_grad)\n",
    "                    meta_layer_outputs.append((weight_output, bias_output))\n",
    "                \n",
    "                # Step 5: Update the main model's parameters using the meta-layer NN output\n",
    "                with torch.no_grad():\n",
    "                    for param, (weight_update, bias_update) in zip(self.main_model.parameters(), meta_layer_outputs):\n",
    "                        param.grad = weight_update if param.grad is None else param.grad.add(weight_update)\n",
    "\n",
    "                # Step 6: Perform backward pass for meta neural networks\n",
    "                meta_loss = loss  # Assuming meta loss is based on the primary model's loss\n",
    "                for i, meta_layer_nn in enumerate(self.meta_layer_nns):\n",
    "                    self.meta_optimizers[i].zero_grad()\n",
    "                    meta_loss.backward(retain_graph=True)\n",
    "                    self.meta_optimizers[i].step()\n",
    "\n",
    "                # Step 7: Track training loss and accuracy for this batch\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Calculate metrics for the epoch\n",
    "            epoch_loss = running_loss / len(self.trainloader)\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "            epoch_duration = time.time() - start_time\n",
    "            self.training_losses.append(epoch_loss)\n",
    "            self.training_accuracies.append(epoch_accuracy)\n",
    "            self.epoch_times.append(epoch_duration)\n",
    "\n",
    "            # Validation phase to monitor performance on validation data\n",
    "            val_loss, val_accuracy = self.validate()\n",
    "            self.validation_losses.append(val_loss)\n",
    "            self.validation_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.early_stop_count = 0\n",
    "            else:\n",
    "                self.early_stop_count += 1\n",
    "                if self.early_stop_count >= self.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"Epoch [{epoch+1}/{self.num_epochs}] \"\n",
    "                  f\"Training Loss: {epoch_loss:.4f} \"\n",
    "                  f\"Training Acc: {epoch_accuracy:.2f}% \"\n",
    "                  f\"Validation Loss: {val_loss:.4f} \"\n",
    "                  f\"Validation Acc: {val_accuracy:.2f}% \"\n",
    "                  f\"Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.main_model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.validationloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.main_model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        val_loss = running_loss / len(self.validationloader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        return val_loss, val_accuracy\n",
    "\n",
    "    def test(self):\n",
    "        self.main_model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.testloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.main_model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Calculate test metrics\n",
    "        test_loss = running_loss / len(self.testloader)\n",
    "        test_accuracy = 100 * correct / total\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = initialize_main_model()\n",
    "optimizers = {\n",
    "    \"adam\": initialize_adam_optimizer(),\n",
    "    \"sgd\": initialize_sgd_optimizer()\n",
    "}\n",
    "meta_layer_nns = initialize_meta_layer_nns(main_model, len(optimizers))\n",
    "meta_optimizers = [initialize_adam_optimizer() for _ in range(len(meta_layer_nns))]\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    for batch in training_data:\n",
    "        # step 1: forward pass and loss computation for the main model\n",
    "        predictions = main_model.forward(batch.input)\n",
    "        loss = compute_loss(predictions, batch.labels)\n",
    "        # step 2: compute raw gradients\n",
    "        raw_gradients = compute_gradients(loss, main_model.parameters)\n",
    "        # step 3: preprocess gradients using optimizers\n",
    "        gradients_optimizers = []\n",
    "        for optimizer in optimizers.values:\n",
    "            gradients_optimizer = optimizer.preprocess_gradients(raw_gradients)\n",
    "            gradients_optimizers.append(gradients_optimizer)\n",
    "        \n",
    "        # step 4: here we have to iterate through each layer's gradients\n",
    "        # as far as I undestand, gradients_optimizers is of shape [num_optimizers, gradients]\n",
    "        # where gradients store weight, biases pairs for each layer\n",
    "        # something like [weights_layer1, biases_layer1, weights_layer2, biases_layer2, etc.]\n",
    "        # so we have to iterate for each layer, pass each layer gradients for weights and biases to according meta_layer_nn\n",
    "        # we store the outputs of each meta_layer_nn\n",
    "        for meta_layer_nn in meta_layer_nns:\n",
    "            # ...\n",
    "        meta_layer_nns_output = # combined outputs of each meta_layer_nn, which is of shape same as gradients_optimizer\n",
    "        \n",
    "        # step 5: update the primary model's parameters using the Meta NN output\n",
    "        primary_model.update_parameters(meta_layer_nns_output)\n",
    "        \n",
    "        # step 6: compute the loss for the Meta NN (based on primary model loss)\n",
    "        meta_loss = compute_meta_loss(primary_model.loss)\n",
    "        \n",
    "        # step 7: backward pass for each Meta layer NN and update its parameters\n",
    "        for i, meta_layer_nn in enumerate(meta_layer_nns):\n",
    "            meta_layer_nn.backward(meta_loss)\n",
    "            meta_optimizers[i].step()\n",
    "            meta_optimizers[i].zero_grad()\n",
    "\n",
    "        # step 9: reset gradients for the primary model's optimizers\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    torch.Tensor(64, 3, 3, 3),   # gradient of conv1_1.weight\n",
    "    torch.Tensor(64),            # gradient of conv1_1.bias\n",
    "    torch.Tensor(64, 64, 3, 3),  # gradient of conv1_2.weight\n",
    "    torch.Tensor(64),            # gradient of conv1_2.bias\n",
    "    torch.Tensor(128, 64, 3, 3), # gradient of conv2_1.weight\n",
    "    torch.Tensor(128),           # gradient of conv2_1.bias\n",
    "    torch.Tensor(128, 128, 3, 3),# gradient of conv2_2.weight\n",
    "    torch.Tensor(128),           # gradient of conv2_2.bias\n",
    "    torch.Tensor(256, 128, 3, 3),# gradient of conv3_1.weight\n",
    "    torch.Tensor(256),           # gradient of conv3_1.bias\n",
    "    torch.Tensor(256, 256, 3, 3),# gradient of conv3_2.weight\n",
    "    torch.Tensor(256),           # gradient of conv3_2.bias\n",
    "    torch.Tensor(256, 256, 3, 3),# gradient of conv3_3.weight\n",
    "    torch.Tensor(256),           # gradient of conv3_3.bias\n",
    "    torch.Tensor(512, 256, 3, 3),# gradient of conv4_1.weight\n",
    "    torch.Tensor(512),           # gradient of conv4_1.bias\n",
    "    torch.Tensor(512, 512, 3, 3),# gradient of conv4_2.weight\n",
    "    torch.Tensor(512),           # gradient of conv4_2.bias\n",
    "    torch.Tensor(512, 512, 3, 3),# gradient of conv4_3.weight\n",
    "    torch.Tensor(512),           # gradient of conv4_3.bias\n",
    "    torch.Tensor(4096, 512),     # gradient of fc1.weight\n",
    "    torch.Tensor(512),           # gradient of fc1.bias\n",
    "    torch.Tensor(512, 512),      # gradient of fc2.weight\n",
    "    torch.Tensor(512),           # gradient of fc2.bias\n",
    "    torch.Tensor(10, 512),       # gradient of fc3.weight\n",
    "    torch.Tensor(10),            # gradient of fc3.bias\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
